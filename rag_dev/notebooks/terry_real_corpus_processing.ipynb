{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terry Real Corpus Processing\n",
    "\n",
    "**Purpose**: Process Terry Real's 3 books into ChromaDB collection for RAG-enhanced AI conversations\n",
    "\n",
    "**Task 2 Requirements**:\n",
    "- ğŸ“š Extract text from Terry Real PDFs systematically\n",
    "- ğŸ”ª Implement semantic chunking for relationship concepts\n",
    "- ğŸ·ï¸ Preserve metadata (book source, chapter, concept type)\n",
    "- ğŸš€ Batch embed all chunks with validated all-MiniLM-L6-v2\n",
    "- âœ… Validate quality - chunk coherence and embedding coverage\n",
    "\n",
    "**Technology Stack**: ChromaDB + all-MiniLM-L6-v2 (validated in Task 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Processing Overview\n",
    "\n",
    "**Source Materials**:\n",
    "1. `terry-real-how-can-i-get-through-to-you.pdf`\n",
    "2. `terry-real-new-rules-of-marriage.pdf`\n",
    "3. `terry-real-us-getting-past-you-and-me.pdf`\n",
    "\n",
    "**Processing Pipeline**:\n",
    "1. **Text Extraction** - Extract clean text from PDFs\n",
    "2. **Content Analysis** - Understand structure and identify chapters\n",
    "3. **Chunking Strategy** - Semantic chunking for relationship concepts\n",
    "4. **Metadata Creation** - Preserve book/chapter/concept information\n",
    "5. **Embedding Generation** - Process with all-MiniLM-L6-v2\n",
    "6. **Quality Validation** - Test retrieval and coherence\n",
    "7. **Performance Testing** - Verify query performance for AI conversations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependencies & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ All dependencies imported successfully\n",
      "ChromaDB version: 1.0.12\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# PDF processing\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Text processing and chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ChromaDB and embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"ğŸ“¦ All dependencies imported successfully\")\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ PDF Directory: D:\\Github\\Relational_Life_Practice\\docs\\Research\\source-materials\\pdf books\n",
      "ğŸ“ ChromaDB Directory: D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "ğŸ—‚ï¸ Collection Name: terry_real_corpus\n",
      "ğŸ”§ Chunk Size: 1000, Overlap: 200\n",
      "ğŸ¤– Embedding Model: all-MiniLM-L6-v2\n",
      "\n",
      "ğŸ“š Found 3 PDF files:\n",
      "   - terry-real-how-can-i-get-through-to-you.pdf\n",
      "   - terry-real-new-rules-of-marriage.pdf\n",
      "   - terry-real-us-getting-past-you-and-me.pdf\n",
      "âœ… All Terry Real PDFs found\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()  # From notebooks/ to project root\n",
    "PDF_DIR = PROJECT_ROOT / \"docs\" / \"Research\" / \"source-materials\" / \"pdf books\"\n",
    "CHROMA_DIR = PROJECT_ROOT / \"rag_dev\" / \"chroma_db\"\n",
    "COLLECTION_NAME = \"terry_real_corpus\"\n",
    "\n",
    "# Processing parameters (we'll optimize these)\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Validated in Task 1\n",
    "\n",
    "print(f\"ğŸ“ PDF Directory: {PDF_DIR}\")\n",
    "print(f\"ğŸ“ ChromaDB Directory: {CHROMA_DIR}\")\n",
    "print(f\"ğŸ—‚ï¸ Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"ğŸ”§ Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"ğŸ¤– Embedding Model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Verify PDF files exist\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nğŸ“š Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"   - {pdf.name}\")\n",
    "    \n",
    "if len(pdf_files) != 3:\n",
    "    print(\"âš ï¸ Expected 3 Terry Real PDFs, please verify file paths\")\n",
    "else:\n",
    "    print(\"âœ… All Terry Real PDFs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing ChromaDB and embedding model...\n",
      "âœ… ChromaDB client initialized at D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "âœ… Embedding model 'all-MiniLM-L6-v2' loaded\n",
      "ğŸ“ Embedding dimension: 384\n",
      "âœ… Embedding dimensions match Task 1 validation: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client and embedding model\n",
    "print(\"ğŸš€ Initializing ChromaDB and embedding model...\")\n",
    "\n",
    "# Create ChromaDB directory if it doesn't exist\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "print(f\"âœ… ChromaDB client initialized at {CHROMA_DIR}\")\n",
    "\n",
    "# Initialize embedding model (same as Task 1 validation)\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"âœ… Embedding model '{EMBEDDING_MODEL}' loaded\")\n",
    "print(f\"ğŸ“ Embedding dimension: {embedder.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Verify this matches our Task 1 validation (should be 384)\n",
    "expected_dim = 384\n",
    "actual_dim = embedder.get_sentence_embedding_dimension()\n",
    "if actual_dim == expected_dim:\n",
    "    print(f\"âœ… Embedding dimensions match Task 1 validation: {actual_dim}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Dimension mismatch! Expected {expected_dim}, got {actual_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Preparing clean environment for terry_real_corpus...\n",
      "ğŸ—‘ï¸ Deleted existing collection 'terry_real_corpus'\n",
      "âœ… Fresh collection 'terry_real_corpus' created\n",
      "ğŸ“Š Collection count: 0 documents\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ENVIRONMENT SETUP COMPLETE\n",
      "âœ… Dependencies loaded\n",
      "âœ… Paths configured and verified\n",
      "âœ… ChromaDB client initialized\n",
      "âœ… Embedding model ready (384 dimensions)\n",
      "âœ… Fresh collection created\n",
      "ğŸš€ Ready for PDF text extraction\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean up any existing collection (for fresh processing)\n",
    "print(f\"ğŸ§¹ Preparing clean environment for {COLLECTION_NAME}...\")\n",
    "\n",
    "try:\n",
    "    existing_collection = client.get_collection(COLLECTION_NAME)\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"ğŸ—‘ï¸ Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ No existing collection to delete: {e}\")\n",
    "\n",
    "# Create fresh collection\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"description\": \"Terry Real's Relational Life Therapy corpus for AI conversations\"}\n",
    ")\n",
    "print(f\"âœ… Fresh collection '{COLLECTION_NAME}' created\")\n",
    "print(f\"ğŸ“Š Collection count: {collection.count()} documents\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"âœ… Dependencies loaded\")\n",
    "print(\"âœ… Paths configured and verified\")\n",
    "print(\"âœ… ChromaDB client initialized\")\n",
    "print(\"âœ… Embedding model ready (384 dimensions)\")\n",
    "print(\"âœ… Fresh collection created\")\n",
    "print(\"ğŸš€ Ready for PDF text extraction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction & Content Analysis\n",
    "\n",
    "**Objective**: Extract and analyze text from Terry Real PDFs to understand structure and optimize chunking strategy\n",
    "\n",
    "**Steps**:\n",
    "1. Test text extraction from one book\n",
    "2. Analyze content structure and chapter organization  \n",
    "3. Identify patterns for semantic chunking\n",
    "4. Validate text quality and readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 1: Test Single PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Testing PDF text extraction...\n",
      "ğŸ“– Testing with: terry-real-how-can-i-get-through-to-you.pdf\n",
      "â±ï¸ Extraction time: 17.15 seconds\n",
      "ğŸ“Š Total characters: 579,103\n",
      "ğŸ“Š Total lines: 12,212\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ FIRST 1000 CHARACTERS:\n",
      "============================================================\n",
      "How Can I Get Through to You?: Closing the\n",
      "Intimacy Gap Between Men and Women\n",
      "\n",
      "Terrence Real\n",
      "\n",
      "2003\n",
      "\n",
      "1\n",
      "\n",
      "\fHow Can I Get Through to You?\n",
      "\n",
      "Reconnecting Men and Womeng\n",
      "\n",
      "Terrence Real\n",
      "\n",
      "SCRIBNER\n",
      "New York London Toronto Sydney Singapore\n",
      "\n",
      "SCRIBNER\n",
      "1230 Avenue of the Americas\n",
      "New York, NY 10020\n",
      "www.SimonandSchuster.com\n",
      "\n",
      "2\n",
      "\n",
      "\fCopyright Â© 2002 by Terrence Real\n",
      "\n",
      "All rights reserved, including the right of reproduction in whole or in part in\n",
      "any form.\n",
      "\n",
      "SCRIBNER and design are trademarks of Macmillan Library Reference USA,\n",
      "Inc., used under license by Simon & Schuster, the publisher of this work.\n",
      "\n",
      "For information about special discounts for bulk purchases, please contact Simon\n",
      "& Schuster Special Sales: 1-800-465-6798 or business@simonandschuster.com\n",
      "\n",
      "DESIGNED BY ERICH HOBBING\n",
      "\n",
      "Text set in Janson\n",
      "\n",
      "Manufactured in the United States of America\n",
      "\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "\n",
      "Library of Congress Cataloging-in-Publication Data is available.\n",
      "\n",
      "ISBN-10: 0-684-86877-6\n",
      "\n",
      "eISBN-13: 978-1-439-10676-1\n",
      "\n",
      "ISBN-13: 978-0-684-868\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test extraction from one Terry Real book first\n",
    "print(\"ğŸ” Testing PDF text extraction...\")\n",
    "\n",
    "# Select first PDF for testing\n",
    "test_pdf = pdf_files[0]\n",
    "print(f\"ğŸ“– Testing with: {test_pdf.name}\")\n",
    "\n",
    "# Extract text from PDF\n",
    "start_time = time.time()\n",
    "raw_text = extract_text(str(test_pdf))\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"â±ï¸ Extraction time: {extraction_time:.2f} seconds\")\n",
    "print(f\"ğŸ“Š Total characters: {len(raw_text):,}\")\n",
    "print(f\"ğŸ“Š Total lines: {len(raw_text.splitlines()):,}\")\n",
    "\n",
    "# Show first 1000 characters to understand structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ FIRST 1000 CHARACTERS:\")\n",
    "print(\"=\"*60)\n",
    "print(raw_text[:1000])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 2: Content Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing content structure...\n",
      "ğŸ“Š Non-empty lines: 9,025\n",
      "\n",
      "ğŸ“š Potential chapter markers found: 17\n",
      "   1. Line 70: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "   2. Line 71: 2. Echo Speaks: Empowering the Woman...\n",
      "   3. Line 72: 3. Bringing Men in from the Cold...\n",
      "   4. Line 73: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "   5. Line 74: 5. The Third Ring: A Conspiracy of Silence...\n",
      "   6. Line 75: 6. The Unspeakable Pain of Collusion...\n",
      "   7. Line 76: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "   8. Line 77: 8. Small Murders : How We Lose Passion...\n",
      "   9. Line 78: 9. A New Model of Love...\n",
      "   10. Line 79: 10. Recovering Real Passion...\n",
      "\n",
      "ğŸ” Analyzing structural patterns...\n",
      "ğŸ“ Short lines (<50 chars): 2,271\n",
      "ğŸ“ Long lines (>200 chars): 1\n",
      "ğŸ“¢ All caps lines: 60\n",
      "\n",
      "ğŸ“¢ Sample all-caps lines (potential headings):\n",
      "   SCRIBNER\n",
      "   SCRIBNER\n",
      "   DESIGNED BY ERICH HOBBING\n",
      "   ISBN-10: 0-684-86877-6\n",
      "   ISBN-13: 978-0-684-86877-6\n"
     ]
    }
   ],
   "source": [
    "# Analyze text structure to understand Terry Real's organization\n",
    "print(\"ğŸ” Analyzing content structure...\")\n",
    "\n",
    "lines = raw_text.splitlines()\n",
    "non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "print(f\"ğŸ“Š Non-empty lines: {len(non_empty_lines):,}\")\n",
    "\n",
    "# Look for chapter patterns\n",
    "chapter_patterns = [\n",
    "    r\"^Chapter\\s+\\d+\",\n",
    "    r\"^CHAPTER\\s+\\d+\", \n",
    "    r\"^\\d+\\.\\s+\",\n",
    "    r\"^Part\\s+\\w+\",\n",
    "    r\"^PART\\s+\\w+\"\n",
    "]\n",
    "\n",
    "potential_chapters = []\n",
    "for i, line in enumerate(non_empty_lines[:200]):  # Check first 200 lines\n",
    "    for pattern in chapter_patterns:\n",
    "        if re.match(pattern, line, re.IGNORECASE):\n",
    "            potential_chapters.append((i, line))\n",
    "\n",
    "print(f\"\\nğŸ“š Potential chapter markers found: {len(potential_chapters)}\")\n",
    "for i, chapter in enumerate(potential_chapters[:10]):  # Show first 10\n",
    "    line_num, text = chapter\n",
    "    print(f\"   {i+1}. Line {line_num}: {text[:80]}...\")\n",
    "\n",
    "# Look for other structural elements\n",
    "print(f\"\\nğŸ” Analyzing structural patterns...\")\n",
    "\n",
    "# Count lines with different characteristics\n",
    "short_lines = [line for line in non_empty_lines if len(line) < 50]\n",
    "long_lines = [line for line in non_empty_lines if len(line) > 200]\n",
    "all_caps_lines = [line for line in non_empty_lines if line.isupper() and len(line) > 5]\n",
    "\n",
    "print(f\"ğŸ“ Short lines (<50 chars): {len(short_lines):,}\")\n",
    "print(f\"ğŸ“ Long lines (>200 chars): {len(long_lines):,}\")\n",
    "print(f\"ğŸ“¢ All caps lines: {len(all_caps_lines):,}\")\n",
    "\n",
    "# Show some examples\n",
    "if all_caps_lines:\n",
    "    print(f\"\\nğŸ“¢ Sample all-caps lines (potential headings):\")\n",
    "    for line in all_caps_lines[:5]:\n",
    "        print(f\"   {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 3: Content Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess text quality and identify any extraction issues\n",
    "print(\"ğŸ” Assessing text extraction quality...\")\n",
    "\n",
    "# Sample paragraphs for readability check\n",
    "sample_paragraphs = []\n",
    "current_paragraph = []\n",
    "\n",
    "for line in non_empty_lines[:500]:  # Check first 500 lines\n",
    "    if len(line) > 100:  # Likely paragraph content\n",
    "        current_paragraph.append(line)\n",
    "    elif current_paragraph:\n",
    "        if len(current_paragraph) >= 2:  # Multi-line paragraph\n",
    "            sample_paragraphs.append(\" \".join(current_paragraph))\n",
    "        current_paragraph = []\n",
    "    \n",
    "    if len(sample_paragraphs) >= 3:  # Get 3 sample paragraphs\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ“– Sample readable paragraphs found: {len(sample_paragraphs)}\")\n",
    "\n",
    "for i, paragraph in enumerate(sample_paragraphs):\n",
    "    print(f\"\\nğŸ“– Sample Paragraph {i+1} ({len(paragraph)} chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(paragraph[:300] + (\"...\" if len(paragraph) > 300 else \"\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Check for common extraction issues\n",
    "issues = []\n",
    "if raw_text.count(\"ï¿½\") > 0:\n",
    "    issues.append(f\"Encoding issues: {raw_text.count('ï¿½')} replacement characters\")\n",
    "\n",
    "if len([line for line in lines if len(line) == 1]) > 100:\n",
    "    issues.append(\"Many single-character lines (possible formatting issues)\")\n",
    "\n",
    "if len(re.findall(r'\\w+\\w+\\w+', raw_text)) < len(raw_text.split()) * 0.8:\n",
    "    issues.append(\"Possible word separation issues\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\nâš ï¸ Potential extraction issues:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   - {issue}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Text extraction quality looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 4: Chunking Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimal chunking strategy for Terry Real content\n",
    "print(\"ğŸ”ª Analyzing optimal chunking strategy...\")\n",
    "\n",
    "# Test current chunking parameters\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Create initial chunks for analysis\n",
    "test_chunks = splitter.split_text(raw_text[:50000])  # Test with first 50k characters\n",
    "\n",
    "print(f\"ğŸ“Š Test chunking results:\")\n",
    "print(f\"   Source text: {50000:,} characters\")\n",
    "print(f\"   Generated chunks: {len(test_chunks):,}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(chunk) for chunk in test_chunks]):.0f} characters\")\n",
    "print(f\"   Min chunk size: {min(len(chunk) for chunk in test_chunks)}\")\n",
    "print(f\"   Max chunk size: {max(len(chunk) for chunk in test_chunks)}\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nğŸ“‹ Sample chunks for quality assessment:\")\n",
    "for i, chunk in enumerate(test_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + (\"...\" if len(chunk) > 200 else \"\"))\n",
    "    print(\"--- End Chunk ---\")\n",
    "\n",
    "# Analyze chunk coherence\n",
    "relationship_terms = [\n",
    "    \"relationship\", \"marriage\", \"partner\", \"couple\", \"intimacy\", \n",
    "    \"communication\", \"conflict\", \"emotion\", \"boundary\", \"repair\",\n",
    "    \"empathy\", \"connection\", \"trust\", \"vulnerability\", \"healing\"\n",
    "]\n",
    "\n",
    "chunks_with_terms = []\n",
    "for chunk in test_chunks[:20]:  # Analyze first 20 chunks\n",
    "    term_count = sum(1 for term in relationship_terms if term.lower() in chunk.lower())\n",
    "    chunks_with_terms.append(term_count)\n",
    "\n",
    "avg_terms_per_chunk = np.mean(chunks_with_terms)\n",
    "print(f\"\\nğŸ” Relationship content analysis:\")\n",
    "print(f\"   Average relationship terms per chunk: {avg_terms_per_chunk:.1f}\")\n",
    "print(f\"   Chunks with 3+ relationship terms: {sum(1 for count in chunks_with_terms if count >= 3)}/{len(chunks_with_terms)}\")\n",
    "\n",
    "if avg_terms_per_chunk >= 2:\n",
    "    print(\"âœ… Good relationship content density in chunks\")\n",
    "else:\n",
    "    print(\"âš ï¸ Consider adjusting chunk size for better content coherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 5: Processing Strategy Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize findings and prepare processing strategy\n",
    "print(\"ğŸ“‹ PROCESSING STRATEGY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ğŸ“– Source Material Analysis:\")\n",
    "print(f\"   Book: {test_pdf.name}\")\n",
    "print(f\"   Total characters: {len(raw_text):,}\")\n",
    "print(f\"   Total lines: {len(raw_text.splitlines()):,}\")\n",
    "print(f\"   Extraction time: {extraction_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Content Structure:\")\n",
    "print(f\"   Potential chapters found: {len(potential_chapters)}\")\n",
    "print(f\"   All-caps headings: {len(all_caps_lines)}\")\n",
    "print(f\"   Text quality: {'Good' if not issues else 'Issues detected'}\")\n",
    "\n",
    "print(f\"\\nğŸ”ª Chunking Strategy:\")\n",
    "print(f\"   Current chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"   Current overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(chunk) for chunk in test_chunks]):.0f} chars\")\n",
    "print(f\"   Relationship content density: {avg_terms_per_chunk:.1f} terms/chunk\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nğŸ’¡ Recommendations:\")\n",
    "if len(potential_chapters) > 0:\n",
    "    print(\"   âœ… Chapter structure detected - can preserve in metadata\")\n",
    "else:\n",
    "    print(\"   âš ï¸ No clear chapter structure - use semantic chunking only\")\n",
    "\n",
    "if avg_terms_per_chunk >= 2:\n",
    "    print(\"   âœ… Current chunk size preserves relationship content well\")\n",
    "else:\n",
    "    print(\"   ğŸ“ Consider increasing chunk size for better content coherence\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready to process all {len(pdf_files)} Terry Real books!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
