{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terry Real Corpus Processing\n",
    "\n",
    "**Purpose**: Process Terry Real's 3 books into ChromaDB collection for RAG-enhanced AI conversations\n",
    "\n",
    "**Task 2 Requirements**:\n",
    "- 📚 Extract text from Terry Real PDFs systematically\n",
    "- 🔪 Implement semantic chunking for relationship concepts\n",
    "- 🏷️ Preserve metadata (book source, chapter, concept type)\n",
    "- 🚀 Batch embed all chunks with validated all-MiniLM-L6-v2\n",
    "- ✅ Validate quality - chunk coherence and embedding coverage\n",
    "\n",
    "**Technology Stack**: ChromaDB + all-MiniLM-L6-v2 (validated in Task 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Processing Overview\n",
    "\n",
    "**Source Materials**:\n",
    "1. `terry-real-how-can-i-get-through-to-you.pdf`\n",
    "2. `terry-real-new-rules-of-marriage.pdf`\n",
    "3. `terry-real-us-getting-past-you-and-me.pdf`\n",
    "\n",
    "**Processing Pipeline**:\n",
    "1. **Text Extraction** - Extract clean text from PDFs\n",
    "2. **Content Analysis** - Understand structure and identify chapters\n",
    "3. **Chunking Strategy** - Semantic chunking for relationship concepts\n",
    "4. **Metadata Creation** - Preserve book/chapter/concept information\n",
    "5. **Embedding Generation** - Process with all-MiniLM-L6-v2\n",
    "6. **Quality Validation** - Test retrieval and coherence\n",
    "7. **Performance Testing** - Verify query performance for AI conversations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependencies & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All dependencies imported successfully\n",
      "ChromaDB version: 1.0.12\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# PDF processing\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Text processing and chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ChromaDB and embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"📦 All dependencies imported successfully\")\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 PDF Directory: D:\\Github\\Relational_Life_Practice\\docs\\Research\\source-materials\\pdf books\n",
      "📁 ChromaDB Directory: D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "🗂️ Collection Name: terry_real_corpus\n",
      "🔧 Chunk Size: 1000, Overlap: 200\n",
      "🤖 Embedding Model: all-MiniLM-L6-v2\n",
      "\n",
      "📚 Found 3 PDF files:\n",
      "   - terry-real-how-can-i-get-through-to-you.pdf\n",
      "   - terry-real-new-rules-of-marriage.pdf\n",
      "   - terry-real-us-getting-past-you-and-me.pdf\n",
      "✅ All Terry Real PDFs found\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()  # From notebooks/ to project root\n",
    "PDF_DIR = PROJECT_ROOT / \"docs\" / \"Research\" / \"source-materials\" / \"pdf books\"\n",
    "CHROMA_DIR = PROJECT_ROOT / \"rag_dev\" / \"chroma_db\"\n",
    "COLLECTION_NAME = \"terry_real_corpus\"\n",
    "\n",
    "# Processing parameters (we'll optimize these)\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Validated in Task 1\n",
    "\n",
    "print(f\"📁 PDF Directory: {PDF_DIR}\")\n",
    "print(f\"📁 ChromaDB Directory: {CHROMA_DIR}\")\n",
    "print(f\"🗂️ Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"🔧 Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"🤖 Embedding Model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Verify PDF files exist\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\n📚 Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"   - {pdf.name}\")\n",
    "    \n",
    "if len(pdf_files) != 3:\n",
    "    print(\"⚠️ Expected 3 Terry Real PDFs, please verify file paths\")\n",
    "else:\n",
    "    print(\"✅ All Terry Real PDFs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing ChromaDB and embedding model...\n",
      "✅ ChromaDB client initialized at D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "✅ Embedding model 'all-MiniLM-L6-v2' loaded\n",
      "📐 Embedding dimension: 384\n",
      "✅ Embedding dimensions match Task 1 validation: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client and embedding model\n",
    "print(\"🚀 Initializing ChromaDB and embedding model...\")\n",
    "\n",
    "# Create ChromaDB directory if it doesn't exist\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "print(f\"✅ ChromaDB client initialized at {CHROMA_DIR}\")\n",
    "\n",
    "# Initialize embedding model (same as Task 1 validation)\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"✅ Embedding model '{EMBEDDING_MODEL}' loaded\")\n",
    "print(f\"📐 Embedding dimension: {embedder.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Verify this matches our Task 1 validation (should be 384)\n",
    "expected_dim = 384\n",
    "actual_dim = embedder.get_sentence_embedding_dimension()\n",
    "if actual_dim == expected_dim:\n",
    "    print(f\"✅ Embedding dimensions match Task 1 validation: {actual_dim}\")\n",
    "else:\n",
    "    print(f\"⚠️ Dimension mismatch! Expected {expected_dim}, got {actual_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Preparing clean environment for terry_real_corpus...\n",
      "🗑️ Deleted existing collection 'terry_real_corpus'\n",
      "✅ Fresh collection 'terry_real_corpus' created\n",
      "📊 Collection count: 0 documents\n",
      "\n",
      "============================================================\n",
      "🎉 ENVIRONMENT SETUP COMPLETE\n",
      "✅ Dependencies loaded\n",
      "✅ Paths configured and verified\n",
      "✅ ChromaDB client initialized\n",
      "✅ Embedding model ready (384 dimensions)\n",
      "✅ Fresh collection created\n",
      "🚀 Ready for PDF text extraction\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean up any existing collection (for fresh processing)\n",
    "print(f\"🧹 Preparing clean environment for {COLLECTION_NAME}...\")\n",
    "\n",
    "try:\n",
    "    existing_collection = client.get_collection(COLLECTION_NAME)\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"🗑️ Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹ️ No existing collection to delete: {e}\")\n",
    "\n",
    "# Create fresh collection\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"description\": \"Terry Real's Relational Life Therapy corpus for AI conversations\"}\n",
    ")\n",
    "print(f\"✅ Fresh collection '{COLLECTION_NAME}' created\")\n",
    "print(f\"📊 Collection count: {collection.count()} documents\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"✅ Dependencies loaded\")\n",
    "print(\"✅ Paths configured and verified\")\n",
    "print(\"✅ ChromaDB client initialized\")\n",
    "print(\"✅ Embedding model ready (384 dimensions)\")\n",
    "print(\"✅ Fresh collection created\")\n",
    "print(\"🚀 Ready for PDF text extraction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction & Content Analysis\n",
    "\n",
    "**Objective**: Extract and analyze text from Terry Real PDFs to understand structure and optimize chunking strategy\n",
    "\n",
    "**Steps**:\n",
    "1. Test text extraction from one book\n",
    "2. Analyze content structure and chapter organization  \n",
    "3. Identify patterns for semantic chunking\n",
    "4. Validate text quality and readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 1: Test Single PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing PDF text extraction...\n",
      "📖 Testing with: terry-real-how-can-i-get-through-to-you.pdf\n",
      "⏱️ Extraction time: 17.15 seconds\n",
      "📊 Total characters: 579,103\n",
      "📊 Total lines: 12,212\n",
      "\n",
      "============================================================\n",
      "📋 FIRST 1000 CHARACTERS:\n",
      "============================================================\n",
      "How Can I Get Through to You?: Closing the\n",
      "Intimacy Gap Between Men and Women\n",
      "\n",
      "Terrence Real\n",
      "\n",
      "2003\n",
      "\n",
      "1\n",
      "\n",
      "\fHow Can I Get Through to You?\n",
      "\n",
      "Reconnecting Men and Womeng\n",
      "\n",
      "Terrence Real\n",
      "\n",
      "SCRIBNER\n",
      "New York London Toronto Sydney Singapore\n",
      "\n",
      "SCRIBNER\n",
      "1230 Avenue of the Americas\n",
      "New York, NY 10020\n",
      "www.SimonandSchuster.com\n",
      "\n",
      "2\n",
      "\n",
      "\fCopyright © 2002 by Terrence Real\n",
      "\n",
      "All rights reserved, including the right of reproduction in whole or in part in\n",
      "any form.\n",
      "\n",
      "SCRIBNER and design are trademarks of Macmillan Library Reference USA,\n",
      "Inc., used under license by Simon & Schuster, the publisher of this work.\n",
      "\n",
      "For information about special discounts for bulk purchases, please contact Simon\n",
      "& Schuster Special Sales: 1-800-465-6798 or business@simonandschuster.com\n",
      "\n",
      "DESIGNED BY ERICH HOBBING\n",
      "\n",
      "Text set in Janson\n",
      "\n",
      "Manufactured in the United States of America\n",
      "\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "\n",
      "Library of Congress Cataloging-in-Publication Data is available.\n",
      "\n",
      "ISBN-10: 0-684-86877-6\n",
      "\n",
      "eISBN-13: 978-1-439-10676-1\n",
      "\n",
      "ISBN-13: 978-0-684-868\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test extraction from one Terry Real book first\n",
    "print(\"🔍 Testing PDF text extraction...\")\n",
    "\n",
    "# Select first PDF for testing\n",
    "test_pdf = pdf_files[0]\n",
    "print(f\"📖 Testing with: {test_pdf.name}\")\n",
    "\n",
    "# Extract text from PDF\n",
    "start_time = time.time()\n",
    "raw_text = extract_text(str(test_pdf))\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️ Extraction time: {extraction_time:.2f} seconds\")\n",
    "print(f\"📊 Total characters: {len(raw_text):,}\")\n",
    "print(f\"📊 Total lines: {len(raw_text.splitlines()):,}\")\n",
    "\n",
    "# Show first 1000 characters to understand structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 FIRST 1000 CHARACTERS:\")\n",
    "print(\"=\"*60)\n",
    "print(raw_text[:1000])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 2: Content Structure Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📘 Advanced Chapter Detection & Content Analysis\n",
    "A comprehensive debugging tool that validates chapter detection across multiple book formats and reveals content structure patterns. Originally developed to solve missing chapters in Terry Real's corpus processing.\n",
    "\n",
    "### 🔍 Core Features:\n",
    "- **Multi-Format Pattern Detection**: Automatically detects chapters using diverse formats:\n",
    "  - Numeric: `\"Chapter 1\"`, `\"CHAPTER 2\"`, `\"3. Title\"`\n",
    "  - Word-based: `\"CHAPTER EIGHT\"`, `\"Chapter Eleven\"`\n",
    "  - Title patterns: First 3 words of actual chapter titles\n",
    "- **Intelligent Number-Word Conversion**: Maps 1-20 to `\"ONE\"`, `\"EIGHT\"`, `\"SEVENTEEN\"`, etc.\n",
    "- **Metadata Integration**: Leverages existing `chapter_metadata` for targeted title searches\n",
    "- **Content Structure Discovery**: Reveals book organization patterns (TOC, main content, appendices)\n",
    "\n",
    "### 📊 Advanced Analysis & Reporting:\n",
    "- **Pattern Effectiveness**: Shows which search strategies work best for each chapter\n",
    "- **Content Density Mapping**: Identifies heavily referenced vs. sparse chapters\n",
    "- **Location Distribution**: Reveals duplicate sections, indexes, and reference areas\n",
    "- **Quality Assurance**: 100% detection validation with detailed coverage metrics\n",
    "\n",
    "### 🎯 Real-World Problem Solved:\n",
    "**Challenge**: Missing \"Chapter 8\" and \"Chapter 11\" due to inconsistent formatting (`\"CHAPTER EIGHT\"` vs `\"CHAPTER 8\"`)\n",
    "**Solution**: Dynamic pattern generation covering all numeric and word-based variations\n",
    "**Result**: Perfect 17/17 chapter detection with comprehensive content mapping\n",
    "\n",
    "### ✅ Enhanced Output Example:\n",
    "```\n",
    "📖 Chapter 8 detection:\n",
    "   Pattern 'CHAPTER\\s+EIGHT\\b' → 2 matches:\n",
    "      Line 3587: CHAPTER EIGHT...\n",
    "   ✅ Found at 3 unique locations\n",
    "\n",
    "📊 SUMMARY: 17/17 chapters detected\n",
    "✅ Chapter 13: 18 locations found (heavily referenced)\n",
    "✅ Chapter 8: 3 locations found (word-format detection)\n",
    "```\n",
    "\n",
    "### 🚀 Use Cases:\n",
    "- **Book Corpus Processing**: Validate complete chapter coverage before chunking\n",
    "- **Content Structure Analysis**: Understand document organization patterns  \n",
    "- **Quality Assurance**: Ensure no missing content in RAG system preparation\n",
    "- **Format Debugging**: Identify inconsistent chapter formatting across documents\n",
    "\n",
    "**Perfect for preprocessing academic texts, technical manuals, and therapeutic literature where complete content coverage is critical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUG: Searching for ALL chapters with multiple patterns...\n",
      "\n",
      "📖 Chapter 1 detection:\n",
      "   Pattern 'CHAPTER\\s+ONE\\b' → 2 matches:\n",
      "      Line  297: CHAPTER ONE...\n",
      "      Line 7921: CHAPTER ONE...\n",
      "   Pattern 'Chapter\\s+ONE\\b' → 2 matches:\n",
      "      Line  297: CHAPTER ONE...\n",
      "      Line 7921: CHAPTER ONE...\n",
      "   Pattern '^1\\.\\s+' → 5 matches:\n",
      "      Line   70: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "      Line 5585: 1. Self-Esteem...\n",
      "   Pattern 'Love\\s+on\\s+the' → 2 matches:\n",
      "      Line   70: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "      Line  298: Love on the Ropes: Men and Women in Crisis...\n",
      "   ✅ Found at 8 unique locations\n",
      "\n",
      "📖 Chapter 2 detection:\n",
      "   Pattern 'CHAPTER\\s+TWO\\b' → 2 matches:\n",
      "      Line  801: CHAPTER TWO...\n",
      "      Line 7938: CHAPTER TWO...\n",
      "   Pattern 'Chapter\\s+TWO\\b' → 2 matches:\n",
      "      Line  801: CHAPTER TWO...\n",
      "      Line 7938: CHAPTER TWO...\n",
      "   Pattern '^2\\.\\s+' → 5 matches:\n",
      "      Line   71: 2. Echo Speaks: Empowering the Woman...\n",
      "      Line 5587: 2. Self-Awareness...\n",
      "   Pattern 'Echo\\s+Speaks:\\s+Empowering' → 2 matches:\n",
      "      Line   71: 2. Echo Speaks: Empowering the Woman...\n",
      "      Line  802: Echo Speaks: Empowering the Woman...\n",
      "   ✅ Found at 8 unique locations\n",
      "\n",
      "📖 Chapter 3 detection:\n",
      "   Pattern 'CHAPTER\\s+THREE\\b' → 2 matches:\n",
      "      Line 1243: CHAPTER THREE...\n",
      "      Line 7969: CHAPTER THREE...\n",
      "   Pattern 'Chapter\\s+THREE\\b' → 2 matches:\n",
      "      Line 1243: CHAPTER THREE...\n",
      "      Line 7969: CHAPTER THREE...\n",
      "   Pattern '^3\\.\\s+' → 5 matches:\n",
      "      Line   72: 3. Bringing Men in from the Cold...\n",
      "      Line 5592: 3. Boundaries...\n",
      "   Pattern 'Bringing\\s+Men\\s+in' → 2 matches:\n",
      "      Line   72: 3. Bringing Men in from the Cold...\n",
      "      Line 1244: Bringing Men in from the Cold...\n",
      "   ✅ Found at 8 unique locations\n",
      "\n",
      "📖 Chapter 4 detection:\n",
      "   Pattern 'CHAPTER\\s+FOUR\\b' → 2 matches:\n",
      "      Line 1690: CHAPTER FOUR...\n",
      "      Line 8015: CHAPTER FOUR...\n",
      "   Pattern 'Chapter\\s+FOUR\\b' → 2 matches:\n",
      "      Line 1690: CHAPTER FOUR...\n",
      "      Line 8015: CHAPTER FOUR...\n",
      "   Pattern '^4\\.\\s+' → 5 matches:\n",
      "      Line   73: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "      Line 5594: 4. Interdependence...\n",
      "   Pattern 'Psychological\\s+Patriarchy:\\s+The' → 2 matches:\n",
      "      Line   73: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "      Line 1691: Psychological Patriarchy: The Dance of Contempt...\n",
      "   ✅ Found at 8 unique locations\n",
      "\n",
      "📖 Chapter 5 detection:\n",
      "   Pattern 'CHAPTER\\s+FIVE\\b' → 2 matches:\n",
      "      Line 2059: CHAPTER FIVE...\n",
      "      Line 8067: CHAPTER FIVE...\n",
      "   Pattern 'Chapter\\s+FIVE\\b' → 2 matches:\n",
      "      Line 2059: CHAPTER FIVE...\n",
      "      Line 8067: CHAPTER FIVE...\n",
      "   Pattern '^5\\.\\s+' → 5 matches:\n",
      "      Line   74: 5. The Third Ring: A Conspiracy of Silence...\n",
      "      Line 5596: 5. Moderation...\n",
      "   Pattern 'The\\s+Third\\s+Ring:' → 2 matches:\n",
      "      Line   74: 5. The Third Ring: A Conspiracy of Silence...\n",
      "      Line 2060: The Third Ring: A Conspiracy of Silence...\n",
      "   ✅ Found at 8 unique locations\n",
      "\n",
      "📖 Chapter 6 detection:\n",
      "   Pattern 'CHAPTER\\s+SIX\\b' → 2 matches:\n",
      "      Line 2394: CHAPTER SIX...\n",
      "      Line 8084: CHAPTER SIX...\n",
      "   Pattern 'Chapter\\s+SIX\\b' → 2 matches:\n",
      "      Line 2394: CHAPTER SIX...\n",
      "      Line 8084: CHAPTER SIX...\n",
      "   Pattern '^6\\.\\s+' → 1 matches:\n",
      "      Line   75: 6. The Unspeakable Pain of Collusion...\n",
      "   Pattern 'The\\s+Unspeakable\\s+Pain' → 2 matches:\n",
      "      Line   75: 6. The Unspeakable Pain of Collusion...\n",
      "      Line 2395: The Unspeakable Pain of Collusion...\n",
      "   ✅ Found at 4 unique locations\n",
      "\n",
      "📖 Chapter 7 detection:\n",
      "   Pattern 'CHAPTER\\s+SEVEN\\b' → 2 matches:\n",
      "      Line 2951: CHAPTER SEVEN...\n",
      "      Line 8108: CHAPTER SEVEN...\n",
      "   Pattern 'Chapter\\s+SEVEN\\b' → 2 matches:\n",
      "      Line 2951: CHAPTER SEVEN...\n",
      "      Line 8108: CHAPTER SEVEN...\n",
      "   Pattern '^7\\.\\s+' → 1 matches:\n",
      "      Line   76: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "   Pattern 'Narcissus\\s+Resigns:\\s+An' → 2 matches:\n",
      "      Line   76: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "      Line 2952: Narcissus Resigns: An Unconventional Therapy...\n",
      "   ✅ Found at 4 unique locations\n",
      "\n",
      "📖 Chapter 8 detection:\n",
      "   Pattern 'CHAPTER\\s+EIGHT\\b' → 2 matches:\n",
      "      Line 3587: CHAPTER EIGHT...\n",
      "      Line 8144: CHAPTER EIGHT...\n",
      "   Pattern 'Chapter\\s+EIGHT\\b' → 2 matches:\n",
      "      Line 3587: CHAPTER EIGHT...\n",
      "      Line 8144: CHAPTER EIGHT...\n",
      "   Pattern '^8\\.\\s+' → 1 matches:\n",
      "      Line   77: 8. Small Murders : How We Lose Passion...\n",
      "   Pattern 'Small\\s+Murders\\s+:' → 1 matches:\n",
      "      Line   77: 8. Small Murders : How We Lose Passion...\n",
      "   ✅ Found at 3 unique locations\n",
      "\n",
      "📖 Chapter 9 detection:\n",
      "   Pattern 'CHAPTER\\s+NINE\\b' → 2 matches:\n",
      "      Line 4139: CHAPTER NINE...\n",
      "      Line 8166: CHAPTER NINE...\n",
      "   Pattern 'Chapter\\s+NINE\\b' → 2 matches:\n",
      "      Line 4139: CHAPTER NINE...\n",
      "      Line 8166: CHAPTER NINE...\n",
      "   Pattern '^9\\.\\s+' → 1 matches:\n",
      "      Line   78: 9. A New Model of Love...\n",
      "   Pattern 'A\\s+New\\s+Model' → 3 matches:\n",
      "      Line   78: 9. A New Model of Love...\n",
      "      Line 4140: A New Model of Love...\n",
      "   ✅ Found at 5 unique locations\n",
      "\n",
      "📖 Chapter 10 detection:\n",
      "   Pattern 'CHAPTER\\s+TEN\\b' → 2 matches:\n",
      "      Line 4565: CHAPTER TEN...\n",
      "      Line 8226: CHAPTER TEN...\n",
      "   Pattern 'Chapter\\s+TEN\\b' → 2 matches:\n",
      "      Line 4565: CHAPTER TEN...\n",
      "      Line 8226: CHAPTER TEN...\n",
      "   Pattern '^10\\.\\s+' → 1 matches:\n",
      "      Line   79: 10. Recovering Real Passion...\n",
      "   Pattern 'Recovering\\s+Real\\s+Passion' → 2 matches:\n",
      "      Line   79: 10. Recovering Real Passion...\n",
      "      Line 4566: Recovering Real Passion...\n",
      "   ✅ Found at 4 unique locations\n",
      "\n",
      "📖 Chapter 11 detection:\n",
      "   Pattern 'CHAPTER\\s+ELEVEN\\b' → 2 matches:\n",
      "      Line 4950: CHAPTER ELEVEN...\n",
      "      Line 8246: CHAPTER ELEVEN...\n",
      "   Pattern 'Chapter\\s+ELEVEN\\b' → 2 matches:\n",
      "      Line 4950: CHAPTER ELEVEN...\n",
      "      Line 8246: CHAPTER ELEVEN...\n",
      "   Pattern '^11\\.\\s+' → 1 matches:\n",
      "      Line   80: 11. Love’s Assassins : Control, Revenge, and Resignation...\n",
      "   Pattern 'Love’s\\s+Assassins\\s+:' → 1 matches:\n",
      "      Line   80: 11. Love’s Assassins : Control, Revenge, and Resignation...\n",
      "   ✅ Found at 3 unique locations\n",
      "\n",
      "📖 Chapter 12 detection:\n",
      "   Pattern 'CHAPTER\\s+TWELVE\\b' → 2 matches:\n",
      "      Line 5381: CHAPTER TWELVE...\n",
      "      Line 8260: CHAPTER TWELVE...\n",
      "   Pattern 'Chapter\\s+TWELVE\\b' → 2 matches:\n",
      "      Line 5381: CHAPTER TWELVE...\n",
      "      Line 8260: CHAPTER TWELVE...\n",
      "   Pattern '^12\\.\\s+' → 1 matches:\n",
      "      Line   81: 12. Intimacy as a Daily Practice...\n",
      "   Pattern 'Intimacy\\s+as\\s+a' → 4 matches:\n",
      "      Line   81: 12. Intimacy as a Daily Practice...\n",
      "      Line 5382: Intimacy as a Daily Practice...\n",
      "   ✅ Found at 6 unique locations\n",
      "\n",
      "📖 Chapter 13 detection:\n",
      "   Pattern 'CHAPTER\\s+THIRTEEN\\b' → 2 matches:\n",
      "      Line 5679: CHAPTER THIRTEEN...\n",
      "      Line 8273: CHAPTER THIRTEEN...\n",
      "   Pattern 'Chapter\\s+THIRTEEN\\b' → 2 matches:\n",
      "      Line 5679: CHAPTER THIRTEEN...\n",
      "      Line 8273: CHAPTER THIRTEEN...\n",
      "   Pattern '^13\\.\\s+' → 1 matches:\n",
      "      Line   82: 13. Relational Esteem...\n",
      "   Pattern 'Relational\\s+Esteem' → 16 matches:\n",
      "      Line   82: 13. Relational Esteem...\n",
      "      Line 5680: Relational Esteem...\n",
      "   ✅ Found at 18 unique locations\n",
      "\n",
      "📖 Chapter 14 detection:\n",
      "   Pattern 'CHAPTER\\s+FOURTEEN\\b' → 2 matches:\n",
      "      Line 6240: CHAPTER FOURTEEN...\n",
      "      Line 8277: CHAPTER FOURTEEN...\n",
      "   Pattern 'Chapter\\s+FOURTEEN\\b' → 2 matches:\n",
      "      Line 6240: CHAPTER FOURTEEN...\n",
      "      Line 8277: CHAPTER FOURTEEN...\n",
      "   Pattern '^14\\.\\s+' → 1 matches:\n",
      "      Line   83: 14. Learning to Speak Relationally...\n",
      "   Pattern 'Learning\\s+to\\s+Speak' → 4 matches:\n",
      "      Line   83: 14. Learning to Speak Relationally...\n",
      "      Line 6241: Learning to Speak Relationally...\n",
      "   ✅ Found at 6 unique locations\n",
      "\n",
      "📖 Chapter 15 detection:\n",
      "   Pattern 'CHAPTER\\s+FIFTEEN\\b' → 2 matches:\n",
      "      Line 6606: CHAPTER FIFTEEN...\n",
      "      Line 8282: CHAPTER FIFTEEN...\n",
      "   Pattern 'Chapter\\s+FIFTEEN\\b' → 2 matches:\n",
      "      Line 6606: CHAPTER FIFTEEN...\n",
      "      Line 8282: CHAPTER FIFTEEN...\n",
      "   Pattern '^15\\.\\s+' → 1 matches:\n",
      "      Line   84: 15. Learning to Listen: Scanning for the Positive...\n",
      "   Pattern 'Learning\\s+to\\s+Listen:' → 2 matches:\n",
      "      Line   84: 15. Learning to Listen: Scanning for the Positive...\n",
      "      Line 6607: Learning to Listen: Scanning for the Positive...\n",
      "   ✅ Found at 4 unique locations\n",
      "\n",
      "📖 Chapter 16 detection:\n",
      "   Pattern 'CHAPTER\\s+SIXTEEN\\b' → 2 matches:\n",
      "      Line 6906: CHAPTER SIXTEEN...\n",
      "      Line 8289: CHAPTER SIXTEEN...\n",
      "   Pattern 'Chapter\\s+SIXTEEN\\b' → 2 matches:\n",
      "      Line 6906: CHAPTER SIXTEEN...\n",
      "      Line 8289: CHAPTER SIXTEEN...\n",
      "   Pattern '^16\\.\\s+' → 1 matches:\n",
      "      Line   85: 16. Staying the Course: Negotiation and Integrity...\n",
      "   Pattern 'Staying\\s+the\\s+Course:' → 2 matches:\n",
      "      Line   85: 16. Staying the Course: Negotiation and Integrity...\n",
      "      Line 6907: Staying the Course: Negotiation and Integrity...\n",
      "   ✅ Found at 4 unique locations\n",
      "\n",
      "📖 Chapter 17 detection:\n",
      "   Pattern 'CHAPTER\\s+SEVENTEEN\\b' → 1 matches:\n",
      "      Line 7323: CHAPTER SEVENTEEN...\n",
      "   Pattern 'Chapter\\s+SEVENTEEN\\b' → 1 matches:\n",
      "      Line 7323: CHAPTER SEVENTEEN...\n",
      "   Pattern '^17\\.\\s+' → 1 matches:\n",
      "      Line   87: 17. What It Takes to Love...\n",
      "   Pattern 'What\\s+It\\s+Takes' → 3 matches:\n",
      "      Line   87: 17. What It Takes to Love...\n",
      "      Line 2995: She crouches down. Ready to break Hera’s curse, if that’s what it takes to save...\n",
      "   ✅ Found at 4 unique locations\n",
      "\n",
      "============================================================\n",
      "📊 COMPREHENSIVE CHAPTER DETECTION SUMMARY\n",
      "============================================================\n",
      "✅ Chapters detected: 17/17\n",
      "❌ Chapters missing: 0/17\n",
      "\n",
      "✅ Successfully detected chapters: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "\n",
      "🎉 ALL CHAPTERS DETECTED! Perfect coverage achieved!\n",
      "\n",
      "📋 Detection details:\n",
      "   ✅ Chapter  1: 8 locations found\n",
      "   ✅ Chapter  2: 8 locations found\n",
      "   ✅ Chapter  3: 8 locations found\n",
      "   ✅ Chapter  4: 8 locations found\n",
      "   ✅ Chapter  5: 8 locations found\n",
      "   ✅ Chapter  6: 4 locations found\n",
      "   ✅ Chapter  7: 4 locations found\n",
      "   ✅ Chapter  8: 3 locations found\n",
      "   ✅ Chapter  9: 5 locations found\n",
      "   ✅ Chapter 10: 4 locations found\n",
      "   ✅ Chapter 11: 3 locations found\n",
      "   ✅ Chapter 12: 6 locations found\n",
      "   ✅ Chapter 13: 18 locations found\n",
      "   ✅ Chapter 14: 6 locations found\n",
      "   ✅ Chapter 15: 4 locations found\n",
      "   ✅ Chapter 16: 4 locations found\n",
      "   ✅ Chapter 17: 4 locations found\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Comprehensive chapter detection for all chapters\n",
    "print(f\"\\n🔍 DEBUG: Searching for ALL chapters with multiple patterns...\")\n",
    "\n",
    "# Helper function to convert numbers to words\n",
    "def num_to_word_debug(num):\n",
    "    words = {\n",
    "        1: \"ONE\", 2: \"TWO\", 3: \"THREE\", 4: \"FOUR\", 5: \"FIVE\",\n",
    "        6: \"SIX\", 7: \"SEVEN\", 8: \"EIGHT\", 9: \"NINE\", 10: \"TEN\",\n",
    "        11: \"ELEVEN\", 12: \"TWELVE\", 13: \"THIRTEEN\", 14: \"FOURTEEN\", 15: \"FIFTEEN\",\n",
    "        16: \"SIXTEEN\", 17: \"SEVENTEEN\", 18: \"EIGHTEEN\", 19: \"NINETEEN\", 20: \"TWENTY\"\n",
    "    }\n",
    "    return words.get(num, str(num))\n",
    "\n",
    "# Create comprehensive search patterns for all chapters\n",
    "all_debug_patterns = {}\n",
    "\n",
    "for chapter_num in range(1, 18):  # Chapters 1-17\n",
    "    chapter_word = num_to_word_debug(chapter_num)\n",
    "    \n",
    "    # Generate multiple pattern variations for each chapter\n",
    "    patterns = [\n",
    "        f\"CHAPTER\\\\s+{chapter_num}\\\\b\",           # \"CHAPTER 1\"\n",
    "        f\"Chapter\\\\s+{chapter_num}\\\\b\",           # \"Chapter 1\"\n",
    "        f\"CHAPTER\\\\s+{chapter_word}\\\\b\",          # \"CHAPTER ONE\"\n",
    "        f\"Chapter\\\\s+{chapter_word}\\\\b\",          # \"Chapter One\"\n",
    "        f\"^{chapter_num}\\\\.\\\\s+\",                 # \"1. \" (start of line)\n",
    "    ]\n",
    "    \n",
    "    # Add chapter-specific title patterns if available\n",
    "    if 'chapter_metadata' in globals():\n",
    "        for ch in chapter_metadata:\n",
    "            if ch['number'] == chapter_num:\n",
    "                # Add first few words of title\n",
    "                title_words = ch['title'].split()[:3]  # First 3 words\n",
    "                title_pattern = \"\\\\s+\".join(re.escape(word) for word in title_words)\n",
    "                patterns.append(title_pattern)\n",
    "                break\n",
    "    \n",
    "    all_debug_patterns[chapter_num] = patterns\n",
    "\n",
    "# Search for each chapter using all patterns\n",
    "chapter_detection_summary = {}\n",
    "\n",
    "for chapter_num, patterns in all_debug_patterns.items():\n",
    "    print(f\"\\n📖 Chapter {chapter_num} detection:\")\n",
    "    chapter_matches = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = []\n",
    "        for i, line in enumerate(non_empty_lines):\n",
    "            if re.search(pattern, line, re.IGNORECASE):\n",
    "                matches.append((i, line[:80]))\n",
    "        \n",
    "        if matches:\n",
    "            print(f\"   Pattern '{pattern}' → {len(matches)} matches:\")\n",
    "            for line_idx, text in matches[:2]:  # Show first 2 per pattern\n",
    "                print(f\"      Line {line_idx:4d}: {text}...\")\n",
    "            chapter_matches.extend(matches)\n",
    "    \n",
    "    # Summary for this chapter\n",
    "    unique_lines = list(set(match[0] for match in chapter_matches))\n",
    "    chapter_detection_summary[chapter_num] = len(unique_lines)\n",
    "    \n",
    "    if len(unique_lines) == 0:\n",
    "        print(f\"   ❌ NO matches found for Chapter {chapter_num}\")\n",
    "    else:\n",
    "        print(f\"   ✅ Found at {len(unique_lines)} unique locations\")\n",
    "\n",
    "# Overall detection summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"📊 COMPREHENSIVE CHAPTER DETECTION SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "detected_chapters = [ch for ch, count in chapter_detection_summary.items() if count > 0]\n",
    "missing_chapters = [ch for ch, count in chapter_detection_summary.items() if count == 0]\n",
    "\n",
    "print(f\"✅ Chapters detected: {len(detected_chapters)}/17\")\n",
    "print(f\"❌ Chapters missing: {len(missing_chapters)}/17\")\n",
    "\n",
    "if detected_chapters:\n",
    "    print(f\"\\n✅ Successfully detected chapters: {detected_chapters}\")\n",
    "\n",
    "if missing_chapters:\n",
    "    print(f\"\\n❌ Missing chapters: {missing_chapters}\")\n",
    "    print(f\"💡 These chapters may need additional search patterns\")\n",
    "else:\n",
    "    print(f\"\\n🎉 ALL CHAPTERS DETECTED! Perfect coverage achieved!\")\n",
    "\n",
    "print(f\"\\n📋 Detection details:\")\n",
    "for ch_num in range(1, 18):\n",
    "    status = \"✅\" if chapter_detection_summary[ch_num] > 0 else \"❌\"\n",
    "    count = chapter_detection_summary[ch_num]\n",
    "    print(f\"   {status} Chapter {ch_num:2d}: {count} locations found\")\n",
    "\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing content structure with enhanced detection...\n",
      "📊 Non-empty lines: 9,025\n",
      "\n",
      "📚 Enhanced chapter detection results: 38 markers found\n",
      "📚 After deduplication: 19 unique markers\n",
      "   X. Title: 17 matches\n",
      "   Part Word: 1 matches\n",
      "   Chapter Word: 1 matches\n",
      "\n",
      "📖 Detected chapters with enhanced metadata:\n",
      "    1. Line  70 [X. Title]: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "    2. Line  71 [X. Title]: 2. Echo Speaks: Empowering the Woman...\n",
      "    3. Line  72 [X. Title]: 3. Bringing Men in from the Cold...\n",
      "    4. Line  73 [X. Title]: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "    5. Line  74 [X. Title]: 5. The Third Ring: A Conspiracy of Silence...\n",
      "    6. Line  75 [X. Title]: 6. The Unspeakable Pain of Collusion...\n",
      "    7. Line  76 [X. Title]: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "    8. Line  77 [X. Title]: 8. Small Murders : How We Lose Passion...\n",
      "    9. Line  78 [X. Title]: 9. A New Model of Love...\n",
      "   10. Line  79 [X. Title]: 10. Recovering Real Passion...\n",
      "   11. Line  80 [X. Title]: 11. Love’s Assassins : Control, Revenge, and Resignation...\n",
      "   12. Line  81 [X. Title]: 12. Intimacy as a Daily Practice...\n",
      "\n",
      "🎯 Terry Real format chapters (X. Title): 17\n",
      "\n",
      "📋 Structured chapter metadata extracted:\n",
      "   Chapter  1: Love on the Ropes : Men and Women in Crisis...\n",
      "   Chapter  2: Echo Speaks: Empowering the Woman...\n",
      "   Chapter  3: Bringing Men in from the Cold...\n",
      "   Chapter  4: Psychological Patriarchy: The Dance of Contempt...\n",
      "   Chapter  5: The Third Ring: A Conspiracy of Silence...\n",
      "   Chapter  6: The Unspeakable Pain of Collusion...\n",
      "   Chapter  7: Narcissus Resigns: An Unconventional Therapy...\n",
      "   Chapter  8: Small Murders : How We Lose Passion...\n",
      "   Chapter  9: A New Model of Love...\n",
      "   Chapter 10: Recovering Real Passion...\n",
      "   Chapter 11: Love’s Assassins : Control, Revenge, and Resignation...\n",
      "   Chapter 12: Intimacy as a Daily Practice...\n",
      "   Chapter 13: Relational Esteem...\n",
      "   Chapter 14: Learning to Speak Relationally...\n",
      "   Chapter 15: Learning to Listen: Scanning for the Positive...\n",
      "   Chapter 16: Staying the Course: Negotiation and Integrity...\n",
      "   Chapter 17: What It Takes to Love...\n",
      "\n",
      "🔍 Locating actual chapter content (beyond TOC) with enhanced patterns...\n",
      "📍 Found 17 actual chapter locations (sorted by position):\n",
      "   Ch  1: Line  297 - CHAPTER ONE...\n",
      "   Ch  2: Line  801 - CHAPTER TWO...\n",
      "   Ch  3: Line 1243 - CHAPTER THREE...\n",
      "   Ch  4: Line 1690 - CHAPTER FOUR...\n",
      "   Ch  5: Line 2059 - CHAPTER FIVE...\n",
      "\n",
      "✅ Using actual chapter locations for boundaries\n",
      "\n",
      "📐 Chapter boundaries for processing:\n",
      "   Ch  1: Lines  297- 801 ( 504 lines) - Love on the Ropes : Men and Women in Crisis...\n",
      "   Ch  2: Lines  801-1243 ( 442 lines) - Echo Speaks: Empowering the Woman...\n",
      "   Ch  3: Lines 1243-1690 ( 447 lines) - Bringing Men in from the Cold...\n",
      "   Ch  4: Lines 1690-2059 ( 369 lines) - Psychological Patriarchy: The Dance of Contem...\n",
      "   Ch  5: Lines 2059-2394 ( 335 lines) - The Third Ring: A Conspiracy of Silence...\n",
      "   Ch  6: Lines 2394-2951 ( 557 lines) - The Unspeakable Pain of Collusion...\n",
      "   Ch  7: Lines 2951-3587 ( 636 lines) - Narcissus Resigns: An Unconventional Therapy...\n",
      "   Ch  8: Lines 3587-4139 ( 552 lines) - Small Murders : How We Lose Passion...\n",
      "   Ch  9: Lines 4139-4565 ( 426 lines) - A New Model of Love...\n",
      "   Ch 10: Lines 4565-4950 ( 385 lines) - Recovering Real Passion...\n",
      "   Ch 11: Lines 4950-5381 ( 431 lines) - Love’s Assassins : Control, Revenge, and Resi...\n",
      "   Ch 12: Lines 5381-5679 ( 298 lines) - Intimacy as a Daily Practice...\n",
      "   Ch 13: Lines 5679-6240 ( 561 lines) - Relational Esteem...\n",
      "   Ch 14: Lines 6240-6606 ( 366 lines) - Learning to Speak Relationally...\n",
      "   Ch 15: Lines 6606-6906 ( 300 lines) - Learning to Listen: Scanning for the Positive...\n",
      "   Ch 16: Lines 6906-7323 ( 417 lines) - Staying the Course: Negotiation and Integrity...\n",
      "   Ch 17: Lines 7323-9025 (1702 lines) - What It Takes to Love...\n",
      "\n",
      "📊 All 17 chapters displayed above\n",
      "\n",
      "📊 Chapter-based processing summary:\n",
      "   Total chapters identified: 17\n",
      "   Total content lines: 8,728\n",
      "   Average lines per chapter: 513\n",
      "   ✅ Chapter boundaries stored for processing pipeline\n"
     ]
    }
   ],
   "source": [
    "# Enhanced analysis with improved chapter detection and processing logic\n",
    "print(\"🔍 Analyzing content structure with enhanced detection...\")\n",
    "\n",
    "lines = raw_text.splitlines()\n",
    "non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "print(f\"📊 Non-empty lines: {len(non_empty_lines):,}\")\n",
    "\n",
    "# Enhanced chapter patterns (including suggested improvements)\n",
    "chapter_patterns = [\n",
    "    r\"^Chapter\\s+\\d+\",           # \"Chapter 1\", \"Chapter 2\"\n",
    "    r\"^CHAPTER\\s+\\d+\",           # \"CHAPTER 1\", \"CHAPTER 2\" \n",
    "    r\"^Chapter\\s+\\w+\",           # \"Chapter One\", \"Chapter Two\"\n",
    "    r\"^CHAPTER\\s+\\w+\",           # \"CHAPTER ONE\", \"CHAPTER EIGHT\" ✅ NEW\n",
    "    r\"^\\d+\\s*\\.\\s+\\w+\",          # \"1. Love on the Ropes\", \"2. Echo Speaks\" (Terry Real's format)\n",
    "    r\"^\\d+\\.\\s+\",                # \"1. \", \"2. \" (simpler version)\n",
    "    r\"^[IVXLCDM]+\\.\",            # \"I.\", \"II.\", \"III.\" (Roman numerals)\n",
    "    r\"^Part\\s+\\w+\",              # \"Part One\", \"Part Two\"\n",
    "    r\"^PART\\s+\\w+\"               # \"PART ONE\", \"PART TWO\"\n",
    "]\n",
    "\n",
    "# Find all potential chapters with enhanced detection\n",
    "potential_chapters = []\n",
    "for i, line in enumerate(non_empty_lines[:300]):  # Check first 300 lines (expanded)\n",
    "    for pattern_idx, pattern in enumerate(chapter_patterns):\n",
    "        if re.match(pattern, line, re.IGNORECASE):\n",
    "            potential_chapters.append({\n",
    "                'line_index': i,\n",
    "                'text': line,\n",
    "                'pattern_type': pattern_idx,\n",
    "                'pattern': pattern\n",
    "            })\n",
    "\n",
    "print(f\"\\n📚 Enhanced chapter detection results: {len(potential_chapters)} markers found\")\n",
    "\n",
    "# After finding potential_chapters, add deduplication:\n",
    "# Remove duplicates by keeping only unique line indices\n",
    "seen_lines = set()\n",
    "unique_chapters = []\n",
    "for chapter in potential_chapters:\n",
    "    line_idx = chapter['line_index']\n",
    "    if line_idx not in seen_lines:\n",
    "        seen_lines.add(line_idx)\n",
    "        unique_chapters.append(chapter)\n",
    "\n",
    "potential_chapters = unique_chapters\n",
    "print(f\"📚 After deduplication: {len(potential_chapters)} unique markers\")\n",
    "\n",
    "# Group by pattern type for analysis\n",
    "pattern_counts = Counter([ch['pattern_type'] for ch in potential_chapters])\n",
    "pattern_names = [\n",
    "    \"Chapter X\",        # 0: r\"^Chapter\\s+\\d+\"\n",
    "    \"CHAPTER X\",        # 1: r\"^CHAPTER\\s+\\d+\"\n",
    "    \"Chapter Word\",     # 2: r\"^Chapter\\s+\\w+\" ✅ NEW\n",
    "    \"CHAPTER WORD\",     # 3: r\"^CHAPTER\\s+\\w+\" ✅ NEW\n",
    "    \"X. Title\",         # 4: r\"^\\d+\\s*\\.\\s+\\w+\"\n",
    "    \"X.\",               # 5: r\"^\\d+\\.\\s+\"\n",
    "    \"Roman\",            # 6: r\"^[IVXLCDM]+\\.\"\n",
    "    \"Part Word\",        # 7: r\"^Part\\s+\\w+\"\n",
    "    \"PART WORD\"         # 8: r\"^PART\\s+\\w+\"\n",
    "]\n",
    "\n",
    "for pattern_idx, count in pattern_counts.items():\n",
    "    print(f\"   {pattern_names[pattern_idx]}: {count} matches\")\n",
    "\n",
    "# Show enhanced chapter information\n",
    "print(f\"\\n📖 Detected chapters with enhanced metadata:\")\n",
    "for i, chapter in enumerate(potential_chapters[:12]):  # Show first 12\n",
    "    line_idx = chapter['line_index']\n",
    "    text = chapter['text']\n",
    "    pattern_type = pattern_names[chapter['pattern_type']]\n",
    "    print(f\"   {i+1:2d}. Line {line_idx:3d} [{pattern_type:8s}]: {text[:70]}...\")\n",
    "\n",
    "# Extract chapter titles and numbers for Terry Real's format (pattern type 4: \"X. Title\")\n",
    "terry_real_chapters = [ch for ch in potential_chapters if ch['pattern_type'] == 4]\n",
    "print(f\"\\n🎯 Terry Real format chapters (X. Title): {len(terry_real_chapters)}\")\n",
    "\n",
    "chapter_metadata = []\n",
    "for chapter in terry_real_chapters:\n",
    "    text = chapter['text']\n",
    "    # Extract chapter number and title\n",
    "    match = re.match(r'^(\\d+)\\s*\\.\\s+(.+)', text)\n",
    "    if match:\n",
    "        chapter_num = int(match.group(1))\n",
    "        chapter_title = match.group(2).strip()\n",
    "        chapter_metadata.append({\n",
    "            'number': chapter_num,\n",
    "            'title': chapter_title,\n",
    "            'line_index': chapter['line_index'],\n",
    "            'full_text': text\n",
    "        })\n",
    "\n",
    "# Display structured chapter information\n",
    "if chapter_metadata:\n",
    "    print(f\"\\n📋 Structured chapter metadata extracted:\")\n",
    "    for ch in chapter_metadata:\n",
    "        print(f\"   Chapter {ch['number']:2d}: {ch['title'][:60]}...\")\n",
    "    \n",
    "    # Helper function to convert numbers to words (for word-based chapter search)\n",
    "    def num_to_word(num):\n",
    "        words = {\n",
    "            1: \"ONE\", 2: \"TWO\", 3: \"THREE\", 4: \"FOUR\", 5: \"FIVE\",\n",
    "            6: \"SIX\", 7: \"SEVEN\", 8: \"EIGHT\", 9: \"NINE\", 10: \"TEN\",\n",
    "            11: \"ELEVEN\", 12: \"TWELVE\", 13: \"THIRTEEN\", 14: \"FOURTEEN\", 15: \"FIFTEEN\",\n",
    "            16: \"SIXTEEN\", 17: \"SEVENTEEN\", 18: \"EIGHTEEN\", 19: \"NINETEEN\", 20: \"TWENTY\"\n",
    "        }\n",
    "        return words.get(num, str(num))\n",
    "    \n",
    "    # NEW: Find actual chapter content locations (beyond TOC) with enhanced search\n",
    "    print(f\"\\n🔍 Locating actual chapter content (beyond TOC) with enhanced patterns...\")\n",
    "    \n",
    "    actual_chapter_locations = []\n",
    "    toc_end_line = max(ch['line_index'] for ch in chapter_metadata) + 20  # Start searching after TOC\n",
    "    \n",
    "    for chapter in chapter_metadata:\n",
    "        chapter_title_pattern = re.escape(chapter['title'][:30])  # First 30 chars of title\n",
    "        chapter_num_pattern = f\"^{chapter['number']}\\\\.\"  # Just the number pattern\n",
    "        \n",
    "        # ENHANCED: Add word-based chapter patterns for missing chapters\n",
    "        chapter_word_patterns = [\n",
    "            f\"CHAPTER\\\\s+{num_to_word(chapter['number'])}\",  # CHAPTER EIGHT\n",
    "            f\"Chapter\\\\s+{num_to_word(chapter['number'])}\"   # Chapter Eight\n",
    "        ]\n",
    "        \n",
    "        # Search for this chapter in the main content (after TOC)\n",
    "        found = False\n",
    "        for i, line in enumerate(non_empty_lines[toc_end_line:], start=toc_end_line):\n",
    "            # Original patterns\n",
    "            if (re.search(chapter_title_pattern, line, re.IGNORECASE) or \n",
    "                re.match(chapter_num_pattern, line)):\n",
    "                actual_chapter_locations.append({\n",
    "                    'number': chapter['number'],\n",
    "                    'title': chapter['title'],\n",
    "                    'line_index': i,\n",
    "                    'found_text': line[:100]\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "            \n",
    "            # NEW: Check word-based patterns for chapters like CHAPTER EIGHT\n",
    "            for word_pattern in chapter_word_patterns:\n",
    "                if re.search(word_pattern, line, re.IGNORECASE):\n",
    "                    actual_chapter_locations.append({\n",
    "                        'number': chapter['number'],\n",
    "                        'title': chapter['title'],\n",
    "                        'line_index': i,\n",
    "                        'found_text': line[:100]\n",
    "                    })\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if found:\n",
    "                break\n",
    "    \n",
    "    # CRITICAL FIX: Sort actual chapter locations by line index (not chapter number)\n",
    "    actual_chapter_locations.sort(key=lambda x: x['line_index'])\n",
    "    \n",
    "    print(f\"📍 Found {len(actual_chapter_locations)} actual chapter locations (sorted by position):\")\n",
    "    for loc in actual_chapter_locations[:5]:  # Show first 5\n",
    "        print(f\"   Ch {loc['number']:2d}: Line {loc['line_index']:4d} - {loc['found_text'][:60]}...\")\n",
    "    \n",
    "    # Use actual locations if found, otherwise fall back to TOC\n",
    "    if len(actual_chapter_locations) >= len(chapter_metadata) * 0.5:  # Found at least half\n",
    "        print(f\"\\n✅ Using actual chapter locations for boundaries\")\n",
    "        locations_to_use = actual_chapter_locations\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Using TOC locations (could not find enough actual chapters)\")\n",
    "        locations_to_use = chapter_metadata\n",
    "    \n",
    "    # Create chapter boundaries using the best available locations\n",
    "    chapter_boundaries = []\n",
    "    for i, ch in enumerate(locations_to_use):\n",
    "        start_line = ch['line_index']\n",
    "        end_line = (locations_to_use[i + 1]['line_index'] \n",
    "                   if i + 1 < len(locations_to_use) \n",
    "                   else len(non_empty_lines))\n",
    "        chapter_boundaries.append({\n",
    "            'chapter_num': ch['number'],\n",
    "            'title': ch['title'],\n",
    "            'start_line': start_line,\n",
    "            'end_line': end_line,\n",
    "            'estimated_lines': end_line - start_line\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n📐 Chapter boundaries for processing:\")\n",
    "    total_content_lines = 0\n",
    "    for boundary in chapter_boundaries:  # Show ALL chapters (removed [:8] limit)\n",
    "        lines_count = boundary['estimated_lines']\n",
    "        total_content_lines += lines_count\n",
    "        print(f\"   Ch {boundary['chapter_num']:2d}: Lines {boundary['start_line']:4d}-{boundary['end_line']:4d} ({lines_count:4d} lines) - {boundary['title'][:45]}...\")\n",
    "\n",
    "    # Remove the conditional since we're showing all chapters now\n",
    "    print(f\"\\n📊 All {len(chapter_boundaries)} chapters displayed above\")\n",
    "        \n",
    "    print(f\"\\n📊 Chapter-based processing summary:\")\n",
    "    print(f\"   Total chapters identified: {len(chapter_boundaries)}\")\n",
    "    print(f\"   Total content lines: {sum(b['estimated_lines'] for b in chapter_boundaries):,}\")\n",
    "    print(f\"   Average lines per chapter: {sum(b['estimated_lines'] for b in chapter_boundaries) // len(chapter_boundaries):,}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    globals()['chapter_metadata'] = chapter_metadata\n",
    "    globals()['chapter_boundaries'] = chapter_boundaries\n",
    "    if 'actual_chapter_locations' in locals():\n",
    "        globals()['actual_chapter_locations'] = actual_chapter_locations\n",
    "    print(f\"   ✅ Chapter boundaries stored for processing pipeline\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No Terry Real format chapters detected - will use alternative chunking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 3: Content Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess text quality and identify any extraction issues\n",
    "print(\"🔍 Assessing text extraction quality...\")\n",
    "\n",
    "# Sample paragraphs for readability check\n",
    "sample_paragraphs = []\n",
    "current_paragraph = []\n",
    "\n",
    "for line in non_empty_lines[:500]:  # Check first 500 lines\n",
    "    if len(line) > 100:  # Likely paragraph content\n",
    "        current_paragraph.append(line)\n",
    "    elif current_paragraph:\n",
    "        if len(current_paragraph) >= 2:  # Multi-line paragraph\n",
    "            sample_paragraphs.append(\" \".join(current_paragraph))\n",
    "        current_paragraph = []\n",
    "    \n",
    "    if len(sample_paragraphs) >= 3:  # Get 3 sample paragraphs\n",
    "        break\n",
    "\n",
    "print(f\"📖 Sample readable paragraphs found: {len(sample_paragraphs)}\")\n",
    "\n",
    "for i, paragraph in enumerate(sample_paragraphs):\n",
    "    print(f\"\\n📖 Sample Paragraph {i+1} ({len(paragraph)} chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(paragraph[:300] + (\"...\" if len(paragraph) > 300 else \"\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Check for common extraction issues\n",
    "issues = []\n",
    "if raw_text.count(\"�\") > 0:\n",
    "    issues.append(f\"Encoding issues: {raw_text.count('�')} replacement characters\")\n",
    "\n",
    "if len([line for line in lines if len(line) == 1]) > 100:\n",
    "    issues.append(\"Many single-character lines (possible formatting issues)\")\n",
    "\n",
    "if len(re.findall(r'\\w+\\w+\\w+', raw_text)) < len(raw_text.split()) * 0.8:\n",
    "    issues.append(\"Possible word separation issues\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\n⚠️ Potential extraction issues:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   - {issue}\")\n",
    "else:\n",
    "    print(f\"\\n✅ Text extraction quality looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 4: Chunking Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimal chunking strategy for Terry Real content\n",
    "print(\"🔪 Analyzing optimal chunking strategy...\")\n",
    "\n",
    "# Test current chunking parameters\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Create initial chunks for analysis\n",
    "test_chunks = splitter.split_text(raw_text[:50000])  # Test with first 50k characters\n",
    "\n",
    "print(f\"📊 Test chunking results:\")\n",
    "print(f\"   Source text: {50000:,} characters\")\n",
    "print(f\"   Generated chunks: {len(test_chunks):,}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(chunk) for chunk in test_chunks]):.0f} characters\")\n",
    "print(f\"   Min chunk size: {min(len(chunk) for chunk in test_chunks)}\")\n",
    "print(f\"   Max chunk size: {max(len(chunk) for chunk in test_chunks)}\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\n📋 Sample chunks for quality assessment:\")\n",
    "for i, chunk in enumerate(test_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + (\"...\" if len(chunk) > 200 else \"\"))\n",
    "    print(\"--- End Chunk ---\")\n",
    "\n",
    "# Analyze chunk coherence\n",
    "relationship_terms = [\n",
    "    \"relationship\", \"marriage\", \"partner\", \"couple\", \"intimacy\", \n",
    "    \"communication\", \"conflict\", \"emotion\", \"boundary\", \"repair\",\n",
    "    \"empathy\", \"connection\", \"trust\", \"vulnerability\", \"healing\"\n",
    "]\n",
    "\n",
    "chunks_with_terms = []\n",
    "for chunk in test_chunks[:20]:  # Analyze first 20 chunks\n",
    "    term_count = sum(1 for term in relationship_terms if term.lower() in chunk.lower())\n",
    "    chunks_with_terms.append(term_count)\n",
    "\n",
    "avg_terms_per_chunk = np.mean(chunks_with_terms)\n",
    "print(f\"\\n🔍 Relationship content analysis:\")\n",
    "print(f\"   Average relationship terms per chunk: {avg_terms_per_chunk:.1f}\")\n",
    "print(f\"   Chunks with 3+ relationship terms: {sum(1 for count in chunks_with_terms if count >= 3)}/{len(chunks_with_terms)}\")\n",
    "\n",
    "if avg_terms_per_chunk >= 2:\n",
    "    print(\"✅ Good relationship content density in chunks\")\n",
    "else:\n",
    "    print(\"⚠️ Consider adjusting chunk size for better content coherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 5: Processing Strategy Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize findings and prepare processing strategy\n",
    "print(\"📋 PROCESSING STRATEGY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"📖 Source Material Analysis:\")\n",
    "print(f\"   Book: {test_pdf.name}\")\n",
    "print(f\"   Total characters: {len(raw_text):,}\")\n",
    "print(f\"   Total lines: {len(raw_text.splitlines()):,}\")\n",
    "print(f\"   Extraction time: {extraction_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n🏗️ Content Structure:\")\n",
    "print(f\"   Potential chapters found: {len(potential_chapters)}\")\n",
    "print(f\"   All-caps headings: {len(all_caps_lines)}\")\n",
    "print(f\"   Text quality: {'Good' if not issues else 'Issues detected'}\")\n",
    "\n",
    "print(f\"\\n🔪 Chunking Strategy:\")\n",
    "print(f\"   Current chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"   Current overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(chunk) for chunk in test_chunks]):.0f} chars\")\n",
    "print(f\"   Relationship content density: {avg_terms_per_chunk:.1f} terms/chunk\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "if len(potential_chapters) > 0:\n",
    "    print(\"   ✅ Chapter structure detected - can preserve in metadata\")\n",
    "else:\n",
    "    print(\"   ⚠️ No clear chapter structure - use semantic chunking only\")\n",
    "\n",
    "if avg_terms_per_chunk >= 2:\n",
    "    print(\"   ✅ Current chunk size preserves relationship content well\")\n",
    "else:\n",
    "    print(\"   📝 Consider increasing chunk size for better content coherence\")\n",
    "\n",
    "print(f\"\\n🚀 Ready to process all {len(pdf_files)} Terry Real books!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
