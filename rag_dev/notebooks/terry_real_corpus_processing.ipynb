{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terry Real Corpus Processing\n",
    "\n",
    "**Purpose**: Process Terry Real's 3 books into ChromaDB collection for RAG-enhanced AI conversations\n",
    "\n",
    "**Task 2 Requirements**:\n",
    "- üìö Extract text from Terry Real PDFs systematically\n",
    "- üî™ Implement semantic chunking for relationship concepts\n",
    "- üè∑Ô∏è Preserve metadata (book source, chapter, concept type)\n",
    "- üöÄ Batch embed all chunks with validated all-MiniLM-L6-v2\n",
    "- ‚úÖ Validate quality - chunk coherence and embedding coverage\n",
    "\n",
    "**Technology Stack**: ChromaDB + all-MiniLM-L6-v2 (validated in Task 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Processing Overview\n",
    "\n",
    "**Source Materials**:\n",
    "1. `terry-real-how-can-i-get-through-to-you.pdf`\n",
    "2. `terry-real-new-rules-of-marriage.pdf`\n",
    "3. `terry-real-us-getting-past-you-and-me.pdf`\n",
    "\n",
    "**Processing Pipeline**:\n",
    "1. **Text Extraction** - Extract clean text from PDFs\n",
    "2. **Content Analysis** - Understand structure and identify chapters\n",
    "3. **Chunking Strategy** - Semantic chunking for relationship concepts\n",
    "4. **Metadata Creation** - Preserve book/chapter/concept information\n",
    "5. **Embedding Generation** - Process with all-MiniLM-L6-v2\n",
    "6. **Quality Validation** - Test retrieval and coherence\n",
    "7. **Performance Testing** - Verify query performance for AI conversations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependencies & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ All dependencies imported successfully\n",
      "ChromaDB version: 1.0.12\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# PDF processing\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Text processing and chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ChromaDB and embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"üì¶ All dependencies imported successfully\")\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ PDF Directory: D:\\Github\\Relational_Life_Practice\\docs\\Research\\source-materials\\pdf books\n",
      "üìÅ ChromaDB Directory: D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "üóÇÔ∏è Collection Name: terry_real_corpus\n",
      "üîß Chunk Size: 1000, Overlap: 200\n",
      "ü§ñ Embedding Model: all-MiniLM-L6-v2\n",
      "\n",
      "üìö Found 3 PDF files:\n",
      "   - terry-real-how-can-i-get-through-to-you.pdf\n",
      "   - terry-real-new-rules-of-marriage.pdf\n",
      "   - terry-real-us-getting-past-you-and-me.pdf\n",
      "‚úÖ All Terry Real PDFs found\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()  # From notebooks/ to project root\n",
    "PDF_DIR = PROJECT_ROOT / \"docs\" / \"Research\" / \"source-materials\" / \"pdf books\"\n",
    "CHROMA_DIR = PROJECT_ROOT / \"rag_dev\" / \"chroma_db\"\n",
    "COLLECTION_NAME = \"terry_real_corpus\"\n",
    "\n",
    "# Processing parameters (we'll optimize these)\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Validated in Task 1\n",
    "\n",
    "print(f\"üìÅ PDF Directory: {PDF_DIR}\")\n",
    "print(f\"üìÅ ChromaDB Directory: {CHROMA_DIR}\")\n",
    "print(f\"üóÇÔ∏è Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"üîß Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"ü§ñ Embedding Model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Verify PDF files exist\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nüìö Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"   - {pdf.name}\")\n",
    "    \n",
    "if len(pdf_files) != 3:\n",
    "    print(\"‚ö†Ô∏è Expected 3 Terry Real PDFs, please verify file paths\")\n",
    "else:\n",
    "    print(\"‚úÖ All Terry Real PDFs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing ChromaDB and embedding model...\n",
      "‚úÖ ChromaDB client initialized at D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "‚úÖ Embedding model 'all-MiniLM-L6-v2' loaded\n",
      "üìê Embedding dimension: 384\n",
      "‚úÖ Embedding dimensions match Task 1 validation: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client and embedding model\n",
    "print(\"üöÄ Initializing ChromaDB and embedding model...\")\n",
    "\n",
    "# Create ChromaDB directory if it doesn't exist\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "print(f\"‚úÖ ChromaDB client initialized at {CHROMA_DIR}\")\n",
    "\n",
    "# Initialize embedding model (same as Task 1 validation)\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"‚úÖ Embedding model '{EMBEDDING_MODEL}' loaded\")\n",
    "print(f\"üìê Embedding dimension: {embedder.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Verify this matches our Task 1 validation (should be 384)\n",
    "expected_dim = 384\n",
    "actual_dim = embedder.get_sentence_embedding_dimension()\n",
    "if actual_dim == expected_dim:\n",
    "    print(f\"‚úÖ Embedding dimensions match Task 1 validation: {actual_dim}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Dimension mismatch! Expected {expected_dim}, got {actual_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Preparing clean environment for terry_real_corpus...\n",
      "üóëÔ∏è Deleted existing collection 'terry_real_corpus'\n",
      "‚úÖ Fresh collection 'terry_real_corpus' created\n",
      "üìä Collection count: 0 documents\n",
      "\n",
      "============================================================\n",
      "üéâ ENVIRONMENT SETUP COMPLETE\n",
      "‚úÖ Dependencies loaded\n",
      "‚úÖ Paths configured and verified\n",
      "‚úÖ ChromaDB client initialized\n",
      "‚úÖ Embedding model ready (384 dimensions)\n",
      "‚úÖ Fresh collection created\n",
      "üöÄ Ready for PDF text extraction\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean up any existing collection (for fresh processing)\n",
    "print(f\"üßπ Preparing clean environment for {COLLECTION_NAME}...\")\n",
    "\n",
    "try:\n",
    "    existing_collection = client.get_collection(COLLECTION_NAME)\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"üóëÔ∏è Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è No existing collection to delete: {e}\")\n",
    "\n",
    "# Create fresh collection\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"description\": \"Terry Real's Relational Life Therapy corpus for AI conversations\"}\n",
    ")\n",
    "print(f\"‚úÖ Fresh collection '{COLLECTION_NAME}' created\")\n",
    "print(f\"üìä Collection count: {collection.count()} documents\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"‚úÖ Dependencies loaded\")\n",
    "print(\"‚úÖ Paths configured and verified\")\n",
    "print(\"‚úÖ ChromaDB client initialized\")\n",
    "print(\"‚úÖ Embedding model ready (384 dimensions)\")\n",
    "print(\"‚úÖ Fresh collection created\")\n",
    "print(\"üöÄ Ready for PDF text extraction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction & Content Analysis\n",
    "\n",
    "**Objective**: Extract and analyze text from Terry Real PDFs to understand structure and optimize chunking strategy\n",
    "\n",
    "**Steps**:\n",
    "1. Test text extraction from one book\n",
    "2. Analyze content structure and chapter organization  \n",
    "3. Identify patterns for semantic chunking\n",
    "4. Validate text quality and readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 1: Test Single PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing PDF text extraction...\n",
      "üìñ Testing with: terry-real-how-can-i-get-through-to-you.pdf\n",
      "‚è±Ô∏è Extraction time: 17.15 seconds\n",
      "üìä Total characters: 579,103\n",
      "üìä Total lines: 12,212\n",
      "\n",
      "============================================================\n",
      "üìã FIRST 1000 CHARACTERS:\n",
      "============================================================\n",
      "How Can I Get Through to You?: Closing the\n",
      "Intimacy Gap Between Men and Women\n",
      "\n",
      "Terrence Real\n",
      "\n",
      "2003\n",
      "\n",
      "1\n",
      "\n",
      "\fHow Can I Get Through to You?\n",
      "\n",
      "Reconnecting Men and Womeng\n",
      "\n",
      "Terrence Real\n",
      "\n",
      "SCRIBNER\n",
      "New York London Toronto Sydney Singapore\n",
      "\n",
      "SCRIBNER\n",
      "1230 Avenue of the Americas\n",
      "New York, NY 10020\n",
      "www.SimonandSchuster.com\n",
      "\n",
      "2\n",
      "\n",
      "\fCopyright ¬© 2002 by Terrence Real\n",
      "\n",
      "All rights reserved, including the right of reproduction in whole or in part in\n",
      "any form.\n",
      "\n",
      "SCRIBNER and design are trademarks of Macmillan Library Reference USA,\n",
      "Inc., used under license by Simon & Schuster, the publisher of this work.\n",
      "\n",
      "For information about special discounts for bulk purchases, please contact Simon\n",
      "& Schuster Special Sales: 1-800-465-6798 or business@simonandschuster.com\n",
      "\n",
      "DESIGNED BY ERICH HOBBING\n",
      "\n",
      "Text set in Janson\n",
      "\n",
      "Manufactured in the United States of America\n",
      "\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "\n",
      "Library of Congress Cataloging-in-Publication Data is available.\n",
      "\n",
      "ISBN-10: 0-684-86877-6\n",
      "\n",
      "eISBN-13: 978-1-439-10676-1\n",
      "\n",
      "ISBN-13: 978-0-684-868\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test extraction from one Terry Real book first\n",
    "print(\"üîç Testing PDF text extraction...\")\n",
    "\n",
    "# Select first PDF for testing\n",
    "test_pdf = pdf_files[0]\n",
    "print(f\"üìñ Testing with: {test_pdf.name}\")\n",
    "\n",
    "# Extract text from PDF\n",
    "start_time = time.time()\n",
    "raw_text = extract_text(str(test_pdf))\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Extraction time: {extraction_time:.2f} seconds\")\n",
    "print(f\"üìä Total characters: {len(raw_text):,}\")\n",
    "print(f\"üìä Total lines: {len(raw_text.splitlines()):,}\")\n",
    "\n",
    "# Show first 1000 characters to understand structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã FIRST 1000 CHARACTERS:\")\n",
    "print(\"=\"*60)\n",
    "print(raw_text[:1000])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 2: Content Structure Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò Advanced Chapter Detection & Content Analysis\n",
    "A comprehensive debugging tool that validates chapter detection across multiple book formats and reveals content structure patterns. Originally developed to solve missing chapters in Terry Real's corpus processing.\n",
    "\n",
    "### üîç Core Features:\n",
    "- **Multi-Format Pattern Detection**: Automatically detects chapters using diverse formats:\n",
    "  - Numeric: `\"Chapter 1\"`, `\"CHAPTER 2\"`, `\"3. Title\"`\n",
    "  - Word-based: `\"CHAPTER EIGHT\"`, `\"Chapter Eleven\"`\n",
    "  - Title patterns: First 3 words of actual chapter titles\n",
    "- **Intelligent Number-Word Conversion**: Maps 1-20 to `\"ONE\"`, `\"EIGHT\"`, `\"SEVENTEEN\"`, etc.\n",
    "- **Metadata Integration**: Leverages existing `chapter_metadata` for targeted title searches\n",
    "- **Content Structure Discovery**: Reveals book organization patterns (TOC, main content, appendices)\n",
    "\n",
    "### üìä Advanced Analysis & Reporting:\n",
    "- **Pattern Effectiveness**: Shows which search strategies work best for each chapter\n",
    "- **Content Density Mapping**: Identifies heavily referenced vs. sparse chapters\n",
    "- **Location Distribution**: Reveals duplicate sections, indexes, and reference areas\n",
    "- **Quality Assurance**: 100% detection validation with detailed coverage metrics\n",
    "\n",
    "### üéØ Real-World Problem Solved:\n",
    "**Challenge**: Missing \"Chapter 8\" and \"Chapter 11\" due to inconsistent formatting (`\"CHAPTER EIGHT\"` vs `\"CHAPTER 8\"`)\n",
    "**Solution**: Dynamic pattern generation covering all numeric and word-based variations\n",
    "**Result**: Perfect 17/17 chapter detection with comprehensive content mapping\n",
    "\n",
    "### ‚úÖ Enhanced Output Example:\n",
    "```\n",
    "üìñ Chapter 8 detection:\n",
    "   Pattern 'CHAPTER\\s+EIGHT\\b' ‚Üí 2 matches:\n",
    "      Line 3587: CHAPTER EIGHT...\n",
    "   ‚úÖ Found at 3 unique locations\n",
    "\n",
    "üìä SUMMARY: 17/17 chapters detected\n",
    "‚úÖ Chapter 13: 18 locations found (heavily referenced)\n",
    "‚úÖ Chapter 8: 3 locations found (word-format detection)\n",
    "```\n",
    "\n",
    "### üöÄ Use Cases:\n",
    "- **Book Corpus Processing**: Validate complete chapter coverage before chunking\n",
    "- **Content Structure Analysis**: Understand document organization patterns  \n",
    "- **Quality Assurance**: Ensure no missing content in RAG system preparation\n",
    "- **Format Debugging**: Identify inconsistent chapter formatting across documents\n",
    "\n",
    "**Perfect for preprocessing academic texts, technical manuals, and therapeutic literature where complete content coverage is critical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG: Searching for ALL chapters with multiple patterns...\n",
      "\n",
      "üìñ Chapter 1 detection:\n",
      "   Pattern 'CHAPTER\\s+ONE\\b' ‚Üí 2 matches:\n",
      "      Line  297: CHAPTER ONE...\n",
      "      Line 7921: CHAPTER ONE...\n",
      "   Pattern 'Chapter\\s+ONE\\b' ‚Üí 2 matches:\n",
      "      Line  297: CHAPTER ONE...\n",
      "      Line 7921: CHAPTER ONE...\n",
      "   Pattern '^1\\.\\s+' ‚Üí 5 matches:\n",
      "      Line   70: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "      Line 5585: 1. Self-Esteem...\n",
      "   Pattern 'Love\\s+on\\s+the' ‚Üí 2 matches:\n",
      "      Line   70: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "      Line  298: Love on the Ropes: Men and Women in Crisis...\n",
      "   ‚úÖ Found at 8 unique locations\n",
      "\n",
      "üìñ Chapter 2 detection:\n",
      "   Pattern 'CHAPTER\\s+TWO\\b' ‚Üí 2 matches:\n",
      "      Line  801: CHAPTER TWO...\n",
      "      Line 7938: CHAPTER TWO...\n",
      "   Pattern 'Chapter\\s+TWO\\b' ‚Üí 2 matches:\n",
      "      Line  801: CHAPTER TWO...\n",
      "      Line 7938: CHAPTER TWO...\n",
      "   Pattern '^2\\.\\s+' ‚Üí 5 matches:\n",
      "      Line   71: 2. Echo Speaks: Empowering the Woman...\n",
      "      Line 5587: 2. Self-Awareness...\n",
      "   Pattern 'Echo\\s+Speaks:\\s+Empowering' ‚Üí 2 matches:\n",
      "      Line   71: 2. Echo Speaks: Empowering the Woman...\n",
      "      Line  802: Echo Speaks: Empowering the Woman...\n",
      "   ‚úÖ Found at 8 unique locations\n",
      "\n",
      "üìñ Chapter 3 detection:\n",
      "   Pattern 'CHAPTER\\s+THREE\\b' ‚Üí 2 matches:\n",
      "      Line 1243: CHAPTER THREE...\n",
      "      Line 7969: CHAPTER THREE...\n",
      "   Pattern 'Chapter\\s+THREE\\b' ‚Üí 2 matches:\n",
      "      Line 1243: CHAPTER THREE...\n",
      "      Line 7969: CHAPTER THREE...\n",
      "   Pattern '^3\\.\\s+' ‚Üí 5 matches:\n",
      "      Line   72: 3. Bringing Men in from the Cold...\n",
      "      Line 5592: 3. Boundaries...\n",
      "   Pattern 'Bringing\\s+Men\\s+in' ‚Üí 2 matches:\n",
      "      Line   72: 3. Bringing Men in from the Cold...\n",
      "      Line 1244: Bringing Men in from the Cold...\n",
      "   ‚úÖ Found at 8 unique locations\n",
      "\n",
      "üìñ Chapter 4 detection:\n",
      "   Pattern 'CHAPTER\\s+FOUR\\b' ‚Üí 2 matches:\n",
      "      Line 1690: CHAPTER FOUR...\n",
      "      Line 8015: CHAPTER FOUR...\n",
      "   Pattern 'Chapter\\s+FOUR\\b' ‚Üí 2 matches:\n",
      "      Line 1690: CHAPTER FOUR...\n",
      "      Line 8015: CHAPTER FOUR...\n",
      "   Pattern '^4\\.\\s+' ‚Üí 5 matches:\n",
      "      Line   73: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "      Line 5594: 4. Interdependence...\n",
      "   Pattern 'Psychological\\s+Patriarchy:\\s+The' ‚Üí 2 matches:\n",
      "      Line   73: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "      Line 1691: Psychological Patriarchy: The Dance of Contempt...\n",
      "   ‚úÖ Found at 8 unique locations\n",
      "\n",
      "üìñ Chapter 5 detection:\n",
      "   Pattern 'CHAPTER\\s+FIVE\\b' ‚Üí 2 matches:\n",
      "      Line 2059: CHAPTER FIVE...\n",
      "      Line 8067: CHAPTER FIVE...\n",
      "   Pattern 'Chapter\\s+FIVE\\b' ‚Üí 2 matches:\n",
      "      Line 2059: CHAPTER FIVE...\n",
      "      Line 8067: CHAPTER FIVE...\n",
      "   Pattern '^5\\.\\s+' ‚Üí 5 matches:\n",
      "      Line   74: 5. The Third Ring: A Conspiracy of Silence...\n",
      "      Line 5596: 5. Moderation...\n",
      "   Pattern 'The\\s+Third\\s+Ring:' ‚Üí 2 matches:\n",
      "      Line   74: 5. The Third Ring: A Conspiracy of Silence...\n",
      "      Line 2060: The Third Ring: A Conspiracy of Silence...\n",
      "   ‚úÖ Found at 8 unique locations\n",
      "\n",
      "üìñ Chapter 6 detection:\n",
      "   Pattern 'CHAPTER\\s+SIX\\b' ‚Üí 2 matches:\n",
      "      Line 2394: CHAPTER SIX...\n",
      "      Line 8084: CHAPTER SIX...\n",
      "   Pattern 'Chapter\\s+SIX\\b' ‚Üí 2 matches:\n",
      "      Line 2394: CHAPTER SIX...\n",
      "      Line 8084: CHAPTER SIX...\n",
      "   Pattern '^6\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   75: 6. The Unspeakable Pain of Collusion...\n",
      "   Pattern 'The\\s+Unspeakable\\s+Pain' ‚Üí 2 matches:\n",
      "      Line   75: 6. The Unspeakable Pain of Collusion...\n",
      "      Line 2395: The Unspeakable Pain of Collusion...\n",
      "   ‚úÖ Found at 4 unique locations\n",
      "\n",
      "üìñ Chapter 7 detection:\n",
      "   Pattern 'CHAPTER\\s+SEVEN\\b' ‚Üí 2 matches:\n",
      "      Line 2951: CHAPTER SEVEN...\n",
      "      Line 8108: CHAPTER SEVEN...\n",
      "   Pattern 'Chapter\\s+SEVEN\\b' ‚Üí 2 matches:\n",
      "      Line 2951: CHAPTER SEVEN...\n",
      "      Line 8108: CHAPTER SEVEN...\n",
      "   Pattern '^7\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   76: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "   Pattern 'Narcissus\\s+Resigns:\\s+An' ‚Üí 2 matches:\n",
      "      Line   76: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "      Line 2952: Narcissus Resigns: An Unconventional Therapy...\n",
      "   ‚úÖ Found at 4 unique locations\n",
      "\n",
      "üìñ Chapter 8 detection:\n",
      "   Pattern 'CHAPTER\\s+EIGHT\\b' ‚Üí 2 matches:\n",
      "      Line 3587: CHAPTER EIGHT...\n",
      "      Line 8144: CHAPTER EIGHT...\n",
      "   Pattern 'Chapter\\s+EIGHT\\b' ‚Üí 2 matches:\n",
      "      Line 3587: CHAPTER EIGHT...\n",
      "      Line 8144: CHAPTER EIGHT...\n",
      "   Pattern '^8\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   77: 8. Small Murders : How We Lose Passion...\n",
      "   Pattern 'Small\\s+Murders\\s+:' ‚Üí 1 matches:\n",
      "      Line   77: 8. Small Murders : How We Lose Passion...\n",
      "   ‚úÖ Found at 3 unique locations\n",
      "\n",
      "üìñ Chapter 9 detection:\n",
      "   Pattern 'CHAPTER\\s+NINE\\b' ‚Üí 2 matches:\n",
      "      Line 4139: CHAPTER NINE...\n",
      "      Line 8166: CHAPTER NINE...\n",
      "   Pattern 'Chapter\\s+NINE\\b' ‚Üí 2 matches:\n",
      "      Line 4139: CHAPTER NINE...\n",
      "      Line 8166: CHAPTER NINE...\n",
      "   Pattern '^9\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   78: 9. A New Model of Love...\n",
      "   Pattern 'A\\s+New\\s+Model' ‚Üí 3 matches:\n",
      "      Line   78: 9. A New Model of Love...\n",
      "      Line 4140: A New Model of Love...\n",
      "   ‚úÖ Found at 5 unique locations\n",
      "\n",
      "üìñ Chapter 10 detection:\n",
      "   Pattern 'CHAPTER\\s+TEN\\b' ‚Üí 2 matches:\n",
      "      Line 4565: CHAPTER TEN...\n",
      "      Line 8226: CHAPTER TEN...\n",
      "   Pattern 'Chapter\\s+TEN\\b' ‚Üí 2 matches:\n",
      "      Line 4565: CHAPTER TEN...\n",
      "      Line 8226: CHAPTER TEN...\n",
      "   Pattern '^10\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   79: 10. Recovering Real Passion...\n",
      "   Pattern 'Recovering\\s+Real\\s+Passion' ‚Üí 2 matches:\n",
      "      Line   79: 10. Recovering Real Passion...\n",
      "      Line 4566: Recovering Real Passion...\n",
      "   ‚úÖ Found at 4 unique locations\n",
      "\n",
      "üìñ Chapter 11 detection:\n",
      "   Pattern 'CHAPTER\\s+ELEVEN\\b' ‚Üí 2 matches:\n",
      "      Line 4950: CHAPTER ELEVEN...\n",
      "      Line 8246: CHAPTER ELEVEN...\n",
      "   Pattern 'Chapter\\s+ELEVEN\\b' ‚Üí 2 matches:\n",
      "      Line 4950: CHAPTER ELEVEN...\n",
      "      Line 8246: CHAPTER ELEVEN...\n",
      "   Pattern '^11\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   80: 11. Love‚Äôs Assassins : Control, Revenge, and Resignation...\n",
      "   Pattern 'Love‚Äôs\\s+Assassins\\s+:' ‚Üí 1 matches:\n",
      "      Line   80: 11. Love‚Äôs Assassins : Control, Revenge, and Resignation...\n",
      "   ‚úÖ Found at 3 unique locations\n",
      "\n",
      "üìñ Chapter 12 detection:\n",
      "   Pattern 'CHAPTER\\s+TWELVE\\b' ‚Üí 2 matches:\n",
      "      Line 5381: CHAPTER TWELVE...\n",
      "      Line 8260: CHAPTER TWELVE...\n",
      "   Pattern 'Chapter\\s+TWELVE\\b' ‚Üí 2 matches:\n",
      "      Line 5381: CHAPTER TWELVE...\n",
      "      Line 8260: CHAPTER TWELVE...\n",
      "   Pattern '^12\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   81: 12. Intimacy as a Daily Practice...\n",
      "   Pattern 'Intimacy\\s+as\\s+a' ‚Üí 4 matches:\n",
      "      Line   81: 12. Intimacy as a Daily Practice...\n",
      "      Line 5382: Intimacy as a Daily Practice...\n",
      "   ‚úÖ Found at 6 unique locations\n",
      "\n",
      "üìñ Chapter 13 detection:\n",
      "   Pattern 'CHAPTER\\s+THIRTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 5679: CHAPTER THIRTEEN...\n",
      "      Line 8273: CHAPTER THIRTEEN...\n",
      "   Pattern 'Chapter\\s+THIRTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 5679: CHAPTER THIRTEEN...\n",
      "      Line 8273: CHAPTER THIRTEEN...\n",
      "   Pattern '^13\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   82: 13. Relational Esteem...\n",
      "   Pattern 'Relational\\s+Esteem' ‚Üí 16 matches:\n",
      "      Line   82: 13. Relational Esteem...\n",
      "      Line 5680: Relational Esteem...\n",
      "   ‚úÖ Found at 18 unique locations\n",
      "\n",
      "üìñ Chapter 14 detection:\n",
      "   Pattern 'CHAPTER\\s+FOURTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 6240: CHAPTER FOURTEEN...\n",
      "      Line 8277: CHAPTER FOURTEEN...\n",
      "   Pattern 'Chapter\\s+FOURTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 6240: CHAPTER FOURTEEN...\n",
      "      Line 8277: CHAPTER FOURTEEN...\n",
      "   Pattern '^14\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   83: 14. Learning to Speak Relationally...\n",
      "   Pattern 'Learning\\s+to\\s+Speak' ‚Üí 4 matches:\n",
      "      Line   83: 14. Learning to Speak Relationally...\n",
      "      Line 6241: Learning to Speak Relationally...\n",
      "   ‚úÖ Found at 6 unique locations\n",
      "\n",
      "üìñ Chapter 15 detection:\n",
      "   Pattern 'CHAPTER\\s+FIFTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 6606: CHAPTER FIFTEEN...\n",
      "      Line 8282: CHAPTER FIFTEEN...\n",
      "   Pattern 'Chapter\\s+FIFTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 6606: CHAPTER FIFTEEN...\n",
      "      Line 8282: CHAPTER FIFTEEN...\n",
      "   Pattern '^15\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   84: 15. Learning to Listen: Scanning for the Positive...\n",
      "   Pattern 'Learning\\s+to\\s+Listen:' ‚Üí 2 matches:\n",
      "      Line   84: 15. Learning to Listen: Scanning for the Positive...\n",
      "      Line 6607: Learning to Listen: Scanning for the Positive...\n",
      "   ‚úÖ Found at 4 unique locations\n",
      "\n",
      "üìñ Chapter 16 detection:\n",
      "   Pattern 'CHAPTER\\s+SIXTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 6906: CHAPTER SIXTEEN...\n",
      "      Line 8289: CHAPTER SIXTEEN...\n",
      "   Pattern 'Chapter\\s+SIXTEEN\\b' ‚Üí 2 matches:\n",
      "      Line 6906: CHAPTER SIXTEEN...\n",
      "      Line 8289: CHAPTER SIXTEEN...\n",
      "   Pattern '^16\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   85: 16. Staying the Course: Negotiation and Integrity...\n",
      "   Pattern 'Staying\\s+the\\s+Course:' ‚Üí 2 matches:\n",
      "      Line   85: 16. Staying the Course: Negotiation and Integrity...\n",
      "      Line 6907: Staying the Course: Negotiation and Integrity...\n",
      "   ‚úÖ Found at 4 unique locations\n",
      "\n",
      "üìñ Chapter 17 detection:\n",
      "   Pattern 'CHAPTER\\s+SEVENTEEN\\b' ‚Üí 1 matches:\n",
      "      Line 7323: CHAPTER SEVENTEEN...\n",
      "   Pattern 'Chapter\\s+SEVENTEEN\\b' ‚Üí 1 matches:\n",
      "      Line 7323: CHAPTER SEVENTEEN...\n",
      "   Pattern '^17\\.\\s+' ‚Üí 1 matches:\n",
      "      Line   87: 17. What It Takes to Love...\n",
      "   Pattern 'What\\s+It\\s+Takes' ‚Üí 3 matches:\n",
      "      Line   87: 17. What It Takes to Love...\n",
      "      Line 2995: She crouches down. Ready to break Hera‚Äôs curse, if that‚Äôs what it takes to save...\n",
      "   ‚úÖ Found at 4 unique locations\n",
      "\n",
      "============================================================\n",
      "üìä COMPREHENSIVE CHAPTER DETECTION SUMMARY\n",
      "============================================================\n",
      "‚úÖ Chapters detected: 17/17\n",
      "‚ùå Chapters missing: 0/17\n",
      "\n",
      "‚úÖ Successfully detected chapters: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "\n",
      "üéâ ALL CHAPTERS DETECTED! Perfect coverage achieved!\n",
      "\n",
      "üìã Detection details:\n",
      "   ‚úÖ Chapter  1: 8 locations found\n",
      "   ‚úÖ Chapter  2: 8 locations found\n",
      "   ‚úÖ Chapter  3: 8 locations found\n",
      "   ‚úÖ Chapter  4: 8 locations found\n",
      "   ‚úÖ Chapter  5: 8 locations found\n",
      "   ‚úÖ Chapter  6: 4 locations found\n",
      "   ‚úÖ Chapter  7: 4 locations found\n",
      "   ‚úÖ Chapter  8: 3 locations found\n",
      "   ‚úÖ Chapter  9: 5 locations found\n",
      "   ‚úÖ Chapter 10: 4 locations found\n",
      "   ‚úÖ Chapter 11: 3 locations found\n",
      "   ‚úÖ Chapter 12: 6 locations found\n",
      "   ‚úÖ Chapter 13: 18 locations found\n",
      "   ‚úÖ Chapter 14: 6 locations found\n",
      "   ‚úÖ Chapter 15: 4 locations found\n",
      "   ‚úÖ Chapter 16: 4 locations found\n",
      "   ‚úÖ Chapter 17: 4 locations found\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Comprehensive chapter detection for all chapters\n",
    "print(f\"\\nüîç DEBUG: Searching for ALL chapters with multiple patterns...\")\n",
    "\n",
    "# Helper function to convert numbers to words\n",
    "def num_to_word_debug(num):\n",
    "    words = {\n",
    "        1: \"ONE\", 2: \"TWO\", 3: \"THREE\", 4: \"FOUR\", 5: \"FIVE\",\n",
    "        6: \"SIX\", 7: \"SEVEN\", 8: \"EIGHT\", 9: \"NINE\", 10: \"TEN\",\n",
    "        11: \"ELEVEN\", 12: \"TWELVE\", 13: \"THIRTEEN\", 14: \"FOURTEEN\", 15: \"FIFTEEN\",\n",
    "        16: \"SIXTEEN\", 17: \"SEVENTEEN\", 18: \"EIGHTEEN\", 19: \"NINETEEN\", 20: \"TWENTY\"\n",
    "    }\n",
    "    return words.get(num, str(num))\n",
    "\n",
    "# Create comprehensive search patterns for all chapters\n",
    "all_debug_patterns = {}\n",
    "\n",
    "for chapter_num in range(1, 18):  # Chapters 1-17\n",
    "    chapter_word = num_to_word_debug(chapter_num)\n",
    "    \n",
    "    # Generate multiple pattern variations for each chapter\n",
    "    patterns = [\n",
    "        f\"CHAPTER\\\\s+{chapter_num}\\\\b\",           # \"CHAPTER 1\"\n",
    "        f\"Chapter\\\\s+{chapter_num}\\\\b\",           # \"Chapter 1\"\n",
    "        f\"CHAPTER\\\\s+{chapter_word}\\\\b\",          # \"CHAPTER ONE\"\n",
    "        f\"Chapter\\\\s+{chapter_word}\\\\b\",          # \"Chapter One\"\n",
    "        f\"^{chapter_num}\\\\.\\\\s+\",                 # \"1. \" (start of line)\n",
    "    ]\n",
    "    \n",
    "    # Add chapter-specific title patterns if available\n",
    "    if 'chapter_metadata' in globals():\n",
    "        for ch in chapter_metadata:\n",
    "            if ch['number'] == chapter_num:\n",
    "                # Add first few words of title\n",
    "                title_words = ch['title'].split()[:3]  # First 3 words\n",
    "                title_pattern = \"\\\\s+\".join(re.escape(word) for word in title_words)\n",
    "                patterns.append(title_pattern)\n",
    "                break\n",
    "    \n",
    "    all_debug_patterns[chapter_num] = patterns\n",
    "\n",
    "# Search for each chapter using all patterns\n",
    "chapter_detection_summary = {}\n",
    "\n",
    "for chapter_num, patterns in all_debug_patterns.items():\n",
    "    print(f\"\\nüìñ Chapter {chapter_num} detection:\")\n",
    "    chapter_matches = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = []\n",
    "        for i, line in enumerate(non_empty_lines):\n",
    "            if re.search(pattern, line, re.IGNORECASE):\n",
    "                matches.append((i, line[:80]))\n",
    "        \n",
    "        if matches:\n",
    "            print(f\"   Pattern '{pattern}' ‚Üí {len(matches)} matches:\")\n",
    "            for line_idx, text in matches[:2]:  # Show first 2 per pattern\n",
    "                print(f\"      Line {line_idx:4d}: {text}...\")\n",
    "            chapter_matches.extend(matches)\n",
    "    \n",
    "    # Summary for this chapter\n",
    "    unique_lines = list(set(match[0] for match in chapter_matches))\n",
    "    chapter_detection_summary[chapter_num] = len(unique_lines)\n",
    "    \n",
    "    if len(unique_lines) == 0:\n",
    "        print(f\"   ‚ùå NO matches found for Chapter {chapter_num}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Found at {len(unique_lines)} unique locations\")\n",
    "\n",
    "# Overall detection summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä COMPREHENSIVE CHAPTER DETECTION SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "detected_chapters = [ch for ch, count in chapter_detection_summary.items() if count > 0]\n",
    "missing_chapters = [ch for ch, count in chapter_detection_summary.items() if count == 0]\n",
    "\n",
    "print(f\"‚úÖ Chapters detected: {len(detected_chapters)}/17\")\n",
    "print(f\"‚ùå Chapters missing: {len(missing_chapters)}/17\")\n",
    "\n",
    "if detected_chapters:\n",
    "    print(f\"\\n‚úÖ Successfully detected chapters: {detected_chapters}\")\n",
    "\n",
    "if missing_chapters:\n",
    "    print(f\"\\n‚ùå Missing chapters: {missing_chapters}\")\n",
    "    print(f\"üí° These chapters may need additional search patterns\")\n",
    "else:\n",
    "    print(f\"\\nüéâ ALL CHAPTERS DETECTED! Perfect coverage achieved!\")\n",
    "\n",
    "print(f\"\\nüìã Detection details:\")\n",
    "for ch_num in range(1, 18):\n",
    "    status = \"‚úÖ\" if chapter_detection_summary[ch_num] > 0 else \"‚ùå\"\n",
    "    count = chapter_detection_summary[ch_num]\n",
    "    print(f\"   {status} Chapter {ch_num:2d}: {count} locations found\")\n",
    "\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing content structure with enhanced detection...\n",
      "üìä Non-empty lines: 9,025\n",
      "\n",
      "üìö Enhanced chapter detection results: 38 markers found\n",
      "üìö After deduplication: 19 unique markers\n",
      "   X. Title: 17 matches\n",
      "   Part Word: 1 matches\n",
      "   Chapter Word: 1 matches\n",
      "\n",
      "üìñ Detected chapters with enhanced metadata:\n",
      "    1. Line  70 [X. Title]: 1. Love on the Ropes : Men and Women in Crisis...\n",
      "    2. Line  71 [X. Title]: 2. Echo Speaks: Empowering the Woman...\n",
      "    3. Line  72 [X. Title]: 3. Bringing Men in from the Cold...\n",
      "    4. Line  73 [X. Title]: 4. Psychological Patriarchy: The Dance of Contempt...\n",
      "    5. Line  74 [X. Title]: 5. The Third Ring: A Conspiracy of Silence...\n",
      "    6. Line  75 [X. Title]: 6. The Unspeakable Pain of Collusion...\n",
      "    7. Line  76 [X. Title]: 7. Narcissus Resigns: An Unconventional Therapy...\n",
      "    8. Line  77 [X. Title]: 8. Small Murders : How We Lose Passion...\n",
      "    9. Line  78 [X. Title]: 9. A New Model of Love...\n",
      "   10. Line  79 [X. Title]: 10. Recovering Real Passion...\n",
      "   11. Line  80 [X. Title]: 11. Love‚Äôs Assassins : Control, Revenge, and Resignation...\n",
      "   12. Line  81 [X. Title]: 12. Intimacy as a Daily Practice...\n",
      "\n",
      "üéØ Terry Real format chapters (X. Title): 17\n",
      "\n",
      "üìã Structured chapter metadata extracted:\n",
      "   Chapter  1: Love on the Ropes : Men and Women in Crisis...\n",
      "   Chapter  2: Echo Speaks: Empowering the Woman...\n",
      "   Chapter  3: Bringing Men in from the Cold...\n",
      "   Chapter  4: Psychological Patriarchy: The Dance of Contempt...\n",
      "   Chapter  5: The Third Ring: A Conspiracy of Silence...\n",
      "   Chapter  6: The Unspeakable Pain of Collusion...\n",
      "   Chapter  7: Narcissus Resigns: An Unconventional Therapy...\n",
      "   Chapter  8: Small Murders : How We Lose Passion...\n",
      "   Chapter  9: A New Model of Love...\n",
      "   Chapter 10: Recovering Real Passion...\n",
      "   Chapter 11: Love‚Äôs Assassins : Control, Revenge, and Resignation...\n",
      "   Chapter 12: Intimacy as a Daily Practice...\n",
      "   Chapter 13: Relational Esteem...\n",
      "   Chapter 14: Learning to Speak Relationally...\n",
      "   Chapter 15: Learning to Listen: Scanning for the Positive...\n",
      "   Chapter 16: Staying the Course: Negotiation and Integrity...\n",
      "   Chapter 17: What It Takes to Love...\n",
      "\n",
      "üîç Locating actual chapter content (beyond TOC) with enhanced patterns...\n",
      "üìç Found 17 actual chapter locations (sorted by position):\n",
      "   Ch  1: Line  297 - CHAPTER ONE...\n",
      "   Ch  2: Line  801 - CHAPTER TWO...\n",
      "   Ch  3: Line 1243 - CHAPTER THREE...\n",
      "   Ch  4: Line 1690 - CHAPTER FOUR...\n",
      "   Ch  5: Line 2059 - CHAPTER FIVE...\n",
      "\n",
      "‚úÖ Using actual chapter locations for boundaries\n",
      "\n",
      "üìê Chapter boundaries for processing:\n",
      "   Ch  1: Lines  297- 801 ( 504 lines) - Love on the Ropes : Men and Women in Crisis...\n",
      "   Ch  2: Lines  801-1243 ( 442 lines) - Echo Speaks: Empowering the Woman...\n",
      "   Ch  3: Lines 1243-1690 ( 447 lines) - Bringing Men in from the Cold...\n",
      "   Ch  4: Lines 1690-2059 ( 369 lines) - Psychological Patriarchy: The Dance of Contem...\n",
      "   Ch  5: Lines 2059-2394 ( 335 lines) - The Third Ring: A Conspiracy of Silence...\n",
      "   Ch  6: Lines 2394-2951 ( 557 lines) - The Unspeakable Pain of Collusion...\n",
      "   Ch  7: Lines 2951-3587 ( 636 lines) - Narcissus Resigns: An Unconventional Therapy...\n",
      "   Ch  8: Lines 3587-4139 ( 552 lines) - Small Murders : How We Lose Passion...\n",
      "   Ch  9: Lines 4139-4565 ( 426 lines) - A New Model of Love...\n",
      "   Ch 10: Lines 4565-4950 ( 385 lines) - Recovering Real Passion...\n",
      "   Ch 11: Lines 4950-5381 ( 431 lines) - Love‚Äôs Assassins : Control, Revenge, and Resi...\n",
      "   Ch 12: Lines 5381-5679 ( 298 lines) - Intimacy as a Daily Practice...\n",
      "   Ch 13: Lines 5679-6240 ( 561 lines) - Relational Esteem...\n",
      "   Ch 14: Lines 6240-6606 ( 366 lines) - Learning to Speak Relationally...\n",
      "   Ch 15: Lines 6606-6906 ( 300 lines) - Learning to Listen: Scanning for the Positive...\n",
      "   Ch 16: Lines 6906-7323 ( 417 lines) - Staying the Course: Negotiation and Integrity...\n",
      "   Ch 17: Lines 7323-9025 (1702 lines) - What It Takes to Love...\n",
      "\n",
      "üìä All 17 chapters displayed above\n",
      "\n",
      "üìä Chapter-based processing summary:\n",
      "   Total chapters identified: 17\n",
      "   Total content lines: 8,728\n",
      "   Average lines per chapter: 513\n",
      "   ‚úÖ Chapter boundaries stored for processing pipeline\n"
     ]
    }
   ],
   "source": [
    "# Enhanced analysis with improved chapter detection and processing logic\n",
    "print(\"üîç Analyzing content structure with enhanced detection...\")\n",
    "\n",
    "lines = raw_text.splitlines()\n",
    "non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "print(f\"üìä Non-empty lines: {len(non_empty_lines):,}\")\n",
    "\n",
    "# Enhanced chapter patterns (including suggested improvements)\n",
    "chapter_patterns = [\n",
    "    r\"^Chapter\\s+\\d+\",           # \"Chapter 1\", \"Chapter 2\"\n",
    "    r\"^CHAPTER\\s+\\d+\",           # \"CHAPTER 1\", \"CHAPTER 2\" \n",
    "    r\"^Chapter\\s+\\w+\",           # \"Chapter One\", \"Chapter Two\"\n",
    "    r\"^CHAPTER\\s+\\w+\",           # \"CHAPTER ONE\", \"CHAPTER EIGHT\" ‚úÖ NEW\n",
    "    r\"^\\d+\\s*\\.\\s+\\w+\",          # \"1. Love on the Ropes\", \"2. Echo Speaks\" (Terry Real's format)\n",
    "    r\"^\\d+\\.\\s+\",                # \"1. \", \"2. \" (simpler version)\n",
    "    r\"^[IVXLCDM]+\\.\",            # \"I.\", \"II.\", \"III.\" (Roman numerals)\n",
    "    r\"^Part\\s+\\w+\",              # \"Part One\", \"Part Two\"\n",
    "    r\"^PART\\s+\\w+\"               # \"PART ONE\", \"PART TWO\"\n",
    "]\n",
    "\n",
    "# Find all potential chapters with enhanced detection\n",
    "potential_chapters = []\n",
    "for i, line in enumerate(non_empty_lines[:300]):  # Check first 300 lines (expanded)\n",
    "    for pattern_idx, pattern in enumerate(chapter_patterns):\n",
    "        if re.match(pattern, line, re.IGNORECASE):\n",
    "            potential_chapters.append({\n",
    "                'line_index': i,\n",
    "                'text': line,\n",
    "                'pattern_type': pattern_idx,\n",
    "                'pattern': pattern\n",
    "            })\n",
    "\n",
    "print(f\"\\nüìö Enhanced chapter detection results: {len(potential_chapters)} markers found\")\n",
    "\n",
    "# After finding potential_chapters, add deduplication:\n",
    "# Remove duplicates by keeping only unique line indices\n",
    "seen_lines = set()\n",
    "unique_chapters = []\n",
    "for chapter in potential_chapters:\n",
    "    line_idx = chapter['line_index']\n",
    "    if line_idx not in seen_lines:\n",
    "        seen_lines.add(line_idx)\n",
    "        unique_chapters.append(chapter)\n",
    "\n",
    "potential_chapters = unique_chapters\n",
    "print(f\"üìö After deduplication: {len(potential_chapters)} unique markers\")\n",
    "\n",
    "# Group by pattern type for analysis\n",
    "pattern_counts = Counter([ch['pattern_type'] for ch in potential_chapters])\n",
    "pattern_names = [\n",
    "    \"Chapter X\",        # 0: r\"^Chapter\\s+\\d+\"\n",
    "    \"CHAPTER X\",        # 1: r\"^CHAPTER\\s+\\d+\"\n",
    "    \"Chapter Word\",     # 2: r\"^Chapter\\s+\\w+\" ‚úÖ NEW\n",
    "    \"CHAPTER WORD\",     # 3: r\"^CHAPTER\\s+\\w+\" ‚úÖ NEW\n",
    "    \"X. Title\",         # 4: r\"^\\d+\\s*\\.\\s+\\w+\"\n",
    "    \"X.\",               # 5: r\"^\\d+\\.\\s+\"\n",
    "    \"Roman\",            # 6: r\"^[IVXLCDM]+\\.\"\n",
    "    \"Part Word\",        # 7: r\"^Part\\s+\\w+\"\n",
    "    \"PART WORD\"         # 8: r\"^PART\\s+\\w+\"\n",
    "]\n",
    "\n",
    "for pattern_idx, count in pattern_counts.items():\n",
    "    print(f\"   {pattern_names[pattern_idx]}: {count} matches\")\n",
    "\n",
    "# Show enhanced chapter information\n",
    "print(f\"\\nüìñ Detected chapters with enhanced metadata:\")\n",
    "for i, chapter in enumerate(potential_chapters[:12]):  # Show first 12\n",
    "    line_idx = chapter['line_index']\n",
    "    text = chapter['text']\n",
    "    pattern_type = pattern_names[chapter['pattern_type']]\n",
    "    print(f\"   {i+1:2d}. Line {line_idx:3d} [{pattern_type:8s}]: {text[:70]}...\")\n",
    "\n",
    "# Extract chapter titles and numbers for Terry Real's format (pattern type 4: \"X. Title\")\n",
    "terry_real_chapters = [ch for ch in potential_chapters if ch['pattern_type'] == 4]\n",
    "print(f\"\\nüéØ Terry Real format chapters (X. Title): {len(terry_real_chapters)}\")\n",
    "\n",
    "chapter_metadata = []\n",
    "for chapter in terry_real_chapters:\n",
    "    text = chapter['text']\n",
    "    # Extract chapter number and title\n",
    "    match = re.match(r'^(\\d+)\\s*\\.\\s+(.+)', text)\n",
    "    if match:\n",
    "        chapter_num = int(match.group(1))\n",
    "        chapter_title = match.group(2).strip()\n",
    "        chapter_metadata.append({\n",
    "            'number': chapter_num,\n",
    "            'title': chapter_title,\n",
    "            'line_index': chapter['line_index'],\n",
    "            'full_text': text\n",
    "        })\n",
    "\n",
    "# Display structured chapter information\n",
    "if chapter_metadata:\n",
    "    print(f\"\\nüìã Structured chapter metadata extracted:\")\n",
    "    for ch in chapter_metadata:\n",
    "        print(f\"   Chapter {ch['number']:2d}: {ch['title'][:60]}...\")\n",
    "    \n",
    "    # Helper function to convert numbers to words (for word-based chapter search)\n",
    "    def num_to_word(num):\n",
    "        words = {\n",
    "            1: \"ONE\", 2: \"TWO\", 3: \"THREE\", 4: \"FOUR\", 5: \"FIVE\",\n",
    "            6: \"SIX\", 7: \"SEVEN\", 8: \"EIGHT\", 9: \"NINE\", 10: \"TEN\",\n",
    "            11: \"ELEVEN\", 12: \"TWELVE\", 13: \"THIRTEEN\", 14: \"FOURTEEN\", 15: \"FIFTEEN\",\n",
    "            16: \"SIXTEEN\", 17: \"SEVENTEEN\", 18: \"EIGHTEEN\", 19: \"NINETEEN\", 20: \"TWENTY\"\n",
    "        }\n",
    "        return words.get(num, str(num))\n",
    "    \n",
    "    # NEW: Find actual chapter content locations (beyond TOC) with enhanced search\n",
    "    print(f\"\\nüîç Locating actual chapter content (beyond TOC) with enhanced patterns...\")\n",
    "    \n",
    "    actual_chapter_locations = []\n",
    "    toc_end_line = max(ch['line_index'] for ch in chapter_metadata) + 20  # Start searching after TOC\n",
    "    \n",
    "    for chapter in chapter_metadata:\n",
    "        chapter_title_pattern = re.escape(chapter['title'][:30])  # First 30 chars of title\n",
    "        chapter_num_pattern = f\"^{chapter['number']}\\\\.\"  # Just the number pattern\n",
    "        \n",
    "        # ENHANCED: Add word-based chapter patterns for missing chapters\n",
    "        chapter_word_patterns = [\n",
    "            f\"CHAPTER\\\\s+{num_to_word(chapter['number'])}\",  # CHAPTER EIGHT\n",
    "            f\"Chapter\\\\s+{num_to_word(chapter['number'])}\"   # Chapter Eight\n",
    "        ]\n",
    "        \n",
    "        # Search for this chapter in the main content (after TOC)\n",
    "        found = False\n",
    "        for i, line in enumerate(non_empty_lines[toc_end_line:], start=toc_end_line):\n",
    "            # Original patterns\n",
    "            if (re.search(chapter_title_pattern, line, re.IGNORECASE) or \n",
    "                re.match(chapter_num_pattern, line)):\n",
    "                actual_chapter_locations.append({\n",
    "                    'number': chapter['number'],\n",
    "                    'title': chapter['title'],\n",
    "                    'line_index': i,\n",
    "                    'found_text': line[:100]\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "            \n",
    "            # NEW: Check word-based patterns for chapters like CHAPTER EIGHT\n",
    "            for word_pattern in chapter_word_patterns:\n",
    "                if re.search(word_pattern, line, re.IGNORECASE):\n",
    "                    actual_chapter_locations.append({\n",
    "                        'number': chapter['number'],\n",
    "                        'title': chapter['title'],\n",
    "                        'line_index': i,\n",
    "                        'found_text': line[:100]\n",
    "                    })\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if found:\n",
    "                break\n",
    "    \n",
    "    # CRITICAL FIX: Sort actual chapter locations by line index (not chapter number)\n",
    "    actual_chapter_locations.sort(key=lambda x: x['line_index'])\n",
    "    \n",
    "    print(f\"üìç Found {len(actual_chapter_locations)} actual chapter locations (sorted by position):\")\n",
    "    for loc in actual_chapter_locations[:5]:  # Show first 5\n",
    "        print(f\"   Ch {loc['number']:2d}: Line {loc['line_index']:4d} - {loc['found_text'][:60]}...\")\n",
    "    \n",
    "    # Use actual locations if found, otherwise fall back to TOC\n",
    "    if len(actual_chapter_locations) >= len(chapter_metadata) * 0.5:  # Found at least half\n",
    "        print(f\"\\n‚úÖ Using actual chapter locations for boundaries\")\n",
    "        locations_to_use = actual_chapter_locations\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Using TOC locations (could not find enough actual chapters)\")\n",
    "        locations_to_use = chapter_metadata\n",
    "    \n",
    "    # Create chapter boundaries using the best available locations\n",
    "    chapter_boundaries = []\n",
    "    for i, ch in enumerate(locations_to_use):\n",
    "        start_line = ch['line_index']\n",
    "        end_line = (locations_to_use[i + 1]['line_index'] \n",
    "                   if i + 1 < len(locations_to_use) \n",
    "                   else len(non_empty_lines))\n",
    "        chapter_boundaries.append({\n",
    "            'chapter_num': ch['number'],\n",
    "            'title': ch['title'],\n",
    "            'start_line': start_line,\n",
    "            'end_line': end_line,\n",
    "            'estimated_lines': end_line - start_line\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nüìê Chapter boundaries for processing:\")\n",
    "    total_content_lines = 0\n",
    "    for boundary in chapter_boundaries:  # Show ALL chapters (removed [:8] limit)\n",
    "        lines_count = boundary['estimated_lines']\n",
    "        total_content_lines += lines_count\n",
    "        print(f\"   Ch {boundary['chapter_num']:2d}: Lines {boundary['start_line']:4d}-{boundary['end_line']:4d} ({lines_count:4d} lines) - {boundary['title'][:45]}...\")\n",
    "\n",
    "    # Remove the conditional since we're showing all chapters now\n",
    "    print(f\"\\nüìä All {len(chapter_boundaries)} chapters displayed above\")\n",
    "        \n",
    "    print(f\"\\nüìä Chapter-based processing summary:\")\n",
    "    print(f\"   Total chapters identified: {len(chapter_boundaries)}\")\n",
    "    print(f\"   Total content lines: {sum(b['estimated_lines'] for b in chapter_boundaries):,}\")\n",
    "    print(f\"   Average lines per chapter: {sum(b['estimated_lines'] for b in chapter_boundaries) // len(chapter_boundaries):,}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    globals()['chapter_metadata'] = chapter_metadata\n",
    "    globals()['chapter_boundaries'] = chapter_boundaries\n",
    "    if 'actual_chapter_locations' in locals():\n",
    "        globals()['actual_chapter_locations'] = actual_chapter_locations\n",
    "    print(f\"   ‚úÖ Chapter boundaries stored for processing pipeline\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No Terry Real format chapters detected - will use alternative chunking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 3: Content Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess text quality and identify any extraction issues\n",
    "print(\"üîç Assessing text extraction quality...\")\n",
    "\n",
    "# Sample paragraphs for readability check\n",
    "sample_paragraphs = []\n",
    "current_paragraph = []\n",
    "\n",
    "for line in non_empty_lines[:500]:  # Check first 500 lines\n",
    "    if len(line) > 100:  # Likely paragraph content\n",
    "        current_paragraph.append(line)\n",
    "    elif current_paragraph:\n",
    "        if len(current_paragraph) >= 2:  # Multi-line paragraph\n",
    "            sample_paragraphs.append(\" \".join(current_paragraph))\n",
    "        current_paragraph = []\n",
    "    \n",
    "    if len(sample_paragraphs) >= 3:  # Get 3 sample paragraphs\n",
    "        break\n",
    "\n",
    "print(f\"üìñ Sample readable paragraphs found: {len(sample_paragraphs)}\")\n",
    "\n",
    "for i, paragraph in enumerate(sample_paragraphs):\n",
    "    print(f\"\\nüìñ Sample Paragraph {i+1} ({len(paragraph)} chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(paragraph[:300] + (\"...\" if len(paragraph) > 300 else \"\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Check for common extraction issues\n",
    "issues = []\n",
    "if raw_text.count(\"ÔøΩ\") > 0:\n",
    "    issues.append(f\"Encoding issues: {raw_text.count('ÔøΩ')} replacement characters\")\n",
    "\n",
    "if len([line for line in lines if len(line) == 1]) > 100:\n",
    "    issues.append(\"Many single-character lines (possible formatting issues)\")\n",
    "\n",
    "if len(re.findall(r'\\w+\\w+\\w+', raw_text)) < len(raw_text.split()) * 0.8:\n",
    "    issues.append(\"Possible word separation issues\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\n‚ö†Ô∏è Potential extraction issues:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   - {issue}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Text extraction quality looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 4: Chunking Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimal chunking strategy for Terry Real content\n",
    "print(\"üî™ Analyzing optimal chunking strategy...\")\n",
    "\n",
    "# Test current chunking parameters\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Create initial chunks for analysis\n",
    "test_chunks = splitter.split_text(raw_text[:50000])  # Test with first 50k characters\n",
    "\n",
    "print(f\"üìä Test chunking results:\")\n",
    "print(f\"   Source text: {50000:,} characters\")\n",
    "print(f\"   Generated chunks: {len(test_chunks):,}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(chunk) for chunk in test_chunks]):.0f} characters\")\n",
    "print(f\"   Min chunk size: {min(len(chunk) for chunk in test_chunks)}\")\n",
    "print(f\"   Max chunk size: {max(len(chunk) for chunk in test_chunks)}\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nüìã Sample chunks for quality assessment:\")\n",
    "for i, chunk in enumerate(test_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + (\"...\" if len(chunk) > 200 else \"\"))\n",
    "    print(\"--- End Chunk ---\")\n",
    "\n",
    "# Analyze chunk coherence\n",
    "relationship_terms = [\n",
    "    \"relationship\", \"marriage\", \"partner\", \"couple\", \"intimacy\", \n",
    "    \"communication\", \"conflict\", \"emotion\", \"boundary\", \"repair\",\n",
    "    \"empathy\", \"connection\", \"trust\", \"vulnerability\", \"healing\"\n",
    "]\n",
    "\n",
    "chunks_with_terms = []\n",
    "for chunk in test_chunks[:20]:  # Analyze first 20 chunks\n",
    "    term_count = sum(1 for term in relationship_terms if term.lower() in chunk.lower())\n",
    "    chunks_with_terms.append(term_count)\n",
    "\n",
    "avg_terms_per_chunk = np.mean(chunks_with_terms)\n",
    "print(f\"\\nüîç Relationship content analysis:\")\n",
    "print(f\"   Average relationship terms per chunk: {avg_terms_per_chunk:.1f}\")\n",
    "print(f\"   Chunks with 3+ relationship terms: {sum(1 for count in chunks_with_terms if count >= 3)}/{len(chunks_with_terms)}\")\n",
    "\n",
    "if avg_terms_per_chunk >= 2:\n",
    "    print(\"‚úÖ Good relationship content density in chunks\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Consider adjusting chunk size for better content coherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cell 5: Processing Strategy Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize findings and prepare processing strategy\n",
    "print(\"üìã PROCESSING STRATEGY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìñ Source Material Analysis:\")\n",
    "print(f\"   Book: {test_pdf.name}\")\n",
    "print(f\"   Total characters: {len(raw_text):,}\")\n",
    "print(f\"   Total lines: {len(raw_text.splitlines()):,}\")\n",
    "print(f\"   Extraction time: {extraction_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Content Structure:\")\n",
    "print(f\"   Potential chapters found: {len(potential_chapters)}\")\n",
    "print(f\"   All-caps headings: {len(all_caps_lines)}\")\n",
    "print(f\"   Text quality: {'Good' if not issues else 'Issues detected'}\")\n",
    "\n",
    "print(f\"\\nüî™ Chunking Strategy:\")\n",
    "print(f\"   Current chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"   Current overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(chunk) for chunk in test_chunks]):.0f} chars\")\n",
    "print(f\"   Relationship content density: {avg_terms_per_chunk:.1f} terms/chunk\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if len(potential_chapters) > 0:\n",
    "    print(\"   ‚úÖ Chapter structure detected - can preserve in metadata\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No clear chapter structure - use semantic chunking only\")\n",
    "\n",
    "if avg_terms_per_chunk >= 2:\n",
    "    print(\"   ‚úÖ Current chunk size preserves relationship content well\")\n",
    "else:\n",
    "    print(\"   üìù Consider increasing chunk size for better content coherence\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to process all {len(pdf_files)} Terry Real books!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
