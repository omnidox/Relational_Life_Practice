{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terry Real Corpus Processing\n",
    "\n",
    "**Purpose**: Process Terry Real's 3 books into ChromaDB collection for RAG-enhanced AI conversations\n",
    "\n",
    "**Task 2 Requirements**:\n",
    "- 📚 Extract text from Terry Real PDFs systematically\n",
    "- 🔪 Implement semantic chunking for relationship concepts\n",
    "- 🏷️ Preserve metadata (book source, chapter, concept type)\n",
    "- 🚀 Batch embed all chunks with validated all-MiniLM-L6-v2\n",
    "- ✅ Validate quality - chunk coherence and embedding coverage\n",
    "\n",
    "**Technology Stack**: ChromaDB + all-MiniLM-L6-v2 (validated in Task 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Processing Overview\n",
    "\n",
    "**Source Materials**:\n",
    "1. `terry-real-how-can-i-get-through-to-you.pdf`\n",
    "2. `terry-real-new-rules-of-marriage.pdf`\n",
    "3. `terry-real-us-getting-past-you-and-me.pdf`\n",
    "\n",
    "**Processing Pipeline**:\n",
    "1. **Text Extraction** - Extract clean text from PDFs\n",
    "2. **Content Analysis** - Understand structure and identify chapters\n",
    "3. **Chunking Strategy** - Semantic chunking for relationship concepts\n",
    "4. **Metadata Creation** - Preserve book/chapter/concept information\n",
    "5. **Embedding Generation** - Process with all-MiniLM-L6-v2\n",
    "6. **Quality Validation** - Test retrieval and coherence\n",
    "7. **Performance Testing** - Verify query performance for AI conversations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependencies & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All dependencies imported successfully\n",
      "ChromaDB version: 1.0.12\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# PDF processing\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Text processing and chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ChromaDB and embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"📦 All dependencies imported successfully\")\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 PDF Directory: D:\\Github\\Relational_Life_Practice\\docs\\Research\\source-materials\\pdf books\n",
      "📁 ChromaDB Directory: D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "🗂️ Collection Name: terry_real_corpus\n",
      "🔧 Chunk Size: 1000, Overlap: 200\n",
      "🤖 Embedding Model: all-MiniLM-L6-v2\n",
      "\n",
      "📚 Found 3 PDF files:\n",
      "   - terry-real-how-can-i-get-through-to-you.pdf\n",
      "   - terry-real-new-rules-of-marriage.pdf\n",
      "   - terry-real-us-getting-past-you-and-me.pdf\n",
      "✅ All Terry Real PDFs found\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()  # From notebooks/ to project root\n",
    "PDF_DIR = PROJECT_ROOT / \"docs\" / \"Research\" / \"source-materials\" / \"pdf books\"\n",
    "CHROMA_DIR = PROJECT_ROOT / \"rag_dev\" / \"chroma_db\"\n",
    "COLLECTION_NAME = \"terry_real_corpus\"\n",
    "\n",
    "# Processing parameters (we'll optimize these)\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Validated in Task 1\n",
    "\n",
    "print(f\"📁 PDF Directory: {PDF_DIR}\")\n",
    "print(f\"📁 ChromaDB Directory: {CHROMA_DIR}\")\n",
    "print(f\"🗂️ Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"🔧 Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"🤖 Embedding Model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Verify PDF files exist\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\n📚 Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"   - {pdf.name}\")\n",
    "    \n",
    "if len(pdf_files) != 3:\n",
    "    print(\"⚠️ Expected 3 Terry Real PDFs, please verify file paths\")\n",
    "else:\n",
    "    print(\"✅ All Terry Real PDFs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing ChromaDB and embedding model...\n",
      "✅ ChromaDB client initialized at D:\\Github\\Relational_Life_Practice\\rag_dev\\chroma_db\n",
      "✅ Embedding model 'all-MiniLM-L6-v2' loaded\n",
      "📐 Embedding dimension: 384\n",
      "✅ Embedding dimensions match Task 1 validation: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client and embedding model\n",
    "print(\"🚀 Initializing ChromaDB and embedding model...\")\n",
    "\n",
    "# Create ChromaDB directory if it doesn't exist\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "print(f\"✅ ChromaDB client initialized at {CHROMA_DIR}\")\n",
    "\n",
    "# Initialize embedding model (same as Task 1 validation)\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"✅ Embedding model '{EMBEDDING_MODEL}' loaded\")\n",
    "print(f\"📐 Embedding dimension: {embedder.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Verify this matches our Task 1 validation (should be 384)\n",
    "expected_dim = 384\n",
    "actual_dim = embedder.get_sentence_embedding_dimension()\n",
    "if actual_dim == expected_dim:\n",
    "    print(f\"✅ Embedding dimensions match Task 1 validation: {actual_dim}\")\n",
    "else:\n",
    "    print(f\"⚠️ Dimension mismatch! Expected {expected_dim}, got {actual_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Preparing clean environment for terry_real_corpus...\n",
      "🗑️ Deleted existing collection 'terry_real_corpus'\n",
      "✅ Fresh collection 'terry_real_corpus' created\n",
      "📊 Collection count: 0 documents\n",
      "\n",
      "============================================================\n",
      "🎉 ENVIRONMENT SETUP COMPLETE\n",
      "✅ Dependencies loaded\n",
      "✅ Paths configured and verified\n",
      "✅ ChromaDB client initialized\n",
      "✅ Embedding model ready (384 dimensions)\n",
      "✅ Fresh collection created\n",
      "🚀 Ready for PDF text extraction\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean up any existing collection (for fresh processing)\n",
    "print(f\"🧹 Preparing clean environment for {COLLECTION_NAME}...\")\n",
    "\n",
    "try:\n",
    "    existing_collection = client.get_collection(COLLECTION_NAME)\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"🗑️ Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹ️ No existing collection to delete: {e}\")\n",
    "\n",
    "# Create fresh collection\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"description\": \"Terry Real's Relational Life Therapy corpus for AI conversations\"}\n",
    ")\n",
    "print(f\"✅ Fresh collection '{COLLECTION_NAME}' created\")\n",
    "print(f\"📊 Collection count: {collection.count()} documents\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"✅ Dependencies loaded\")\n",
    "print(\"✅ Paths configured and verified\")\n",
    "print(\"✅ ChromaDB client initialized\")\n",
    "print(\"✅ Embedding model ready (384 dimensions)\")\n",
    "print(\"✅ Fresh collection created\")\n",
    "print(\"🚀 Ready for PDF text extraction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction & Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text extraction with one book first\n",
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file with error handling.\"\"\"\n",
    "    try:\n",
    "        print(f\"📖 Extracting text from: {pdf_path.name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        text = extract_text(str(pdf_path))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"⏱️ Extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"📊 Total characters extracted: {len(text):,}\")\n",
    "        print(f\"📊 Total words (approximate): {len(text.split()):,}\")\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting text from {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Start with one book for testing\n",
    "test_book = pdf_files[0]  # First Terry Real book\n",
    "sample_text = extract_pdf_text(test_book)\n",
    "\n",
    "if sample_text:\n",
    "    print(f\"\\n✅ Successfully extracted text from {test_book.name}\")\n",
    "    print(f\"📄 First 500 characters preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(sample_text[:500] + \"...\")\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"❌ Failed to extract text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text structure and identify patterns\n",
    "if sample_text:\n",
    "    print(\"🔍 CONTENT STRUCTURE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split into lines for analysis\n",
    "    lines = sample_text.split('\\n')\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    print(f\"📊 Total lines: {len(lines):,}\")\n",
    "    print(f\"📊 Non-empty lines: {len(non_empty_lines):,}\")\n",
    "    \n",
    "    # Look for chapter patterns\n",
    "    print(\"\\n🔍 Searching for chapter patterns...\")\n",
    "    chapter_patterns = [\n",
    "        r'^Chapter\\s+\\d+',\n",
    "        r'^CHAPTER\\s+\\d+',\n",
    "        r'^\\d+\\.',\n",
    "        r'^Part\\s+\\d+',\n",
    "        r'^PART\\s+\\d+'\n",
    "    ]\n",
    "    \n",
    "    potential_chapters = []\n",
    "    for i, line in enumerate(non_empty_lines[:100]):  # Check first 100 non-empty lines\n",
    "        for pattern in chapter_patterns:\n",
    "            if re.match(pattern, line, re.IGNORECASE):\n",
    "                potential_chapters.append((i, line))\n",
    "                break\n",
    "    \n",
    "    if potential_chapters:\n",
    "        print(f\"📋 Found {len(potential_chapters)} potential chapter headers:\")\n",
    "        for i, (line_num, chapter) in enumerate(potential_chapters[:10]):  # Show first 10\n",
    "            print(f\"   {i+1}. Line {line_num}: {chapter}\")\n",
    "    else:\n",
    "        print(\"ℹ️ No standard chapter patterns found, will analyze structure further\")\n",
    "    \n",
    "    # Look for section breaks and headers\n",
    "    print(\"\\n🔍 Analyzing potential section headers...\")\n",
    "    short_lines = [line for line in non_empty_lines[:200] if len(line) < 80 and len(line) > 5]\n",
    "    print(f\"📋 Found {len(short_lines)} potential headers (first 15):\")\n",
    "    for i, header in enumerate(short_lines[:15]):\n",
    "        print(f\"   {i+1}. {header}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze a meaningful sample for chunking strategy\n",
    "if sample_text:\n",
    "    print(\"📚 CONTENT SAMPLE FOR CHUNKING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find a good content section (skip front matter)\n",
    "    # Look for a section that contains relationship content\n",
    "    relationship_keywords = ['relationship', 'marriage', 'couple', 'partner', 'intimacy', 'communication']\n",
    "    \n",
    "    # Split text into paragraphs\n",
    "    paragraphs = [p.strip() for p in sample_text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    print(f\"📊 Total paragraphs found: {len(paragraphs)}\")\n",
    "    \n",
    "    # Find paragraphs with relationship content\n",
    "    relevant_paragraphs = []\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        if len(para) > 100:  # Substantial paragraphs\n",
    "            keyword_count = sum(1 for keyword in relationship_keywords \n",
    "                               if keyword.lower() in para.lower())\n",
    "            if keyword_count > 0:\n",
    "                relevant_paragraphs.append((i, para, keyword_count))\n",
    "    \n",
    "    print(f\"📋 Found {len(relevant_paragraphs)} paragraphs with relationship content\")\n",
    "    \n",
    "    if relevant_paragraphs:\n",
    "        # Sort by keyword relevance and show top 3\n",
    "        relevant_paragraphs.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        print(\"\\n📖 Top 3 most relevant content samples:\")\n",
    "        for i, (para_num, para, keyword_count) in enumerate(relevant_paragraphs[:3]):\n",
    "            print(f\"\\n--- Sample {i+1} (Paragraph {para_num}, {keyword_count} keywords) ---\")\n",
    "            print(para[:400] + \"...\" if len(para) > 400 else para)\n",
    "            print(f\"📏 Length: {len(para)} characters, {len(para.split())} words\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking strategy with sample content\n",
    "if sample_text and relevant_paragraphs:\n",
    "    print(\"🔪 CHUNKING STRATEGY TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get a substantial sample for chunking tests\n",
    "    test_content = \"\\n\\n\".join([para for _, para, _ in relevant_paragraphs[:5]])\n",
    "    print(f\"📊 Test content: {len(test_content)} characters, {len(test_content.split())} words\")\n",
    "    \n",
    "    # Test different chunking parameters\n",
    "    chunking_configs = [\n",
    "        {\"chunk_size\": 800, \"chunk_overlap\": 100, \"name\": \"Conservative\"},\n",
    "        {\"chunk_size\": 1000, \"chunk_overlap\": 200, \"name\": \"Balanced\"},\n",
    "        {\"chunk_size\": 1200, \"chunk_overlap\": 250, \"name\": \"Larger chunks\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🧪 Testing different chunking strategies:\")\n",
    "    \n",
    "    for config in chunking_configs:\n",
    "        print(f\"\\n--- {config['name']} Strategy ---\")\n",
    "        print(f\"Chunk size: {config['chunk_size']}, Overlap: {config['chunk_overlap']}\")\n",
    "        \n",
    "        # Create text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config['chunk_size'],\n",
    "            chunk_overlap=config['chunk_overlap'],\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Split test content\n",
    "        chunks = text_splitter.split_text(test_content)\n",
    "        \n",
    "        print(f\"📊 Generated {len(chunks)} chunks\")\n",
    "        \n",
    "        # Analyze chunk quality\n",
    "        chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "        avg_length = np.mean(chunk_lengths)\n",
    "        min_length = min(chunk_lengths)\n",
    "        max_length = max(chunk_lengths)\n",
    "        \n",
    "        print(f\"📏 Chunk lengths - Avg: {avg_length:.0f}, Min: {min_length}, Max: {max_length}\")\n",
    "        \n",
    "        # Show first chunk sample\n",
    "        if chunks:\n",
    "            print(f\"📖 First chunk preview:\")\n",
    "            print(f\"{chunks[0][:200]}...\")\n",
    "    \n",
    "    print(\"\\n✅ Chunking strategy analysis complete\")"
   ]
  }
